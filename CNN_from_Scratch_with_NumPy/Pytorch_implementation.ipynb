{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab61fba",
   "metadata": {
    "papermill": {
     "duration": 0.002546,
     "end_time": "2025-07-13T23:28:12.448357",
     "exception": false,
     "start_time": "2025-07-13T23:28:12.445811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LeNet-5 on MNIST with PyTorch\n",
    "\n",
    "Replicate the Numpy model in pytorch and train it on Kaggles GPU T4 x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f16fd6",
   "metadata": {
    "papermill": {
     "duration": 0.001632,
     "end_time": "2025-07-13T23:28:12.452293",
     "exception": false,
     "start_time": "2025-07-13T23:28:12.450661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c2287",
   "metadata": {
    "papermill": {
     "duration": 0.001673,
     "end_time": "2025-07-13T23:28:12.455774",
     "exception": false,
     "start_time": "2025-07-13T23:28:12.454101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Layer      | Type                 | Parameters                             | Output Shape (input 28x28) |\n",
    "| ---------- | -------------------- | -------------------------------------- | -------------------------- |\n",
    "| **C1**     | Convolution          | 6 filters, 5×5 kernel, stride=1, pad=2 | (6, 28, 28)                |\n",
    "|            | Activation (Sigmoid) |                                        | (6, 28, 28)                |\n",
    "| **S2**     | Average Pooling      | 2×2 window, stride=2                   | (6, 14, 14)                |\n",
    "| **C3**     | Convolution          | 16 filters, 5×5 kernel, stride=1       | (16, 10, 10)               |\n",
    "|            | Activation (Sigmoid) |                                        | (16, 10, 10)               |\n",
    "| **S4**     | Average Pooling      | 2×2 window, stride=2                   | (16, 5, 5)                 |\n",
    "| **C5**     | Convolution          | 120 filters, 5×5 kernel, stride=1      | (120, 1, 1)                |\n",
    "|            | Activation (Sigmoid) |                                        | (120,)                     |\n",
    "| **F6**     | Fully Connected      | 120 → 84                               | (84,)                      |\n",
    "|            | Activation (Sigmoid) |                                        | (84,)                      |\n",
    "| **Output** | Fully Connected      | 84 → 10                                | (10,)                      |\n",
    "\n",
    "![Architecture](figures/Architecture.png)\n",
    "\n",
    "Image source: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. - https://github.com/d2l-ai/d2l-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30db33a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T23:28:12.460251Z",
     "iopub.status.busy": "2025-07-13T23:28:12.459979Z",
     "iopub.status.idle": "2025-07-13T23:28:21.191383Z",
     "shell.execute_reply": "2025-07-13T23:28:21.190707Z"
    },
    "papermill": {
     "duration": 8.735192,
     "end_time": "2025-07-13T23:28:21.192759",
     "exception": false,
     "start_time": "2025-07-13T23:28:12.457567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af4d31",
   "metadata": {
    "papermill": {
     "duration": 0.001724,
     "end_time": "2025-07-13T23:28:21.196754",
     "exception": false,
     "start_time": "2025-07-13T23:28:21.195030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Normalize MNIST using the dataset's standard mean (0.1307) and std (0.3081)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f0073a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T23:28:21.201336Z",
     "iopub.status.busy": "2025-07-13T23:28:21.201006Z",
     "iopub.status.idle": "2025-07-13T23:28:46.062519Z",
     "shell.execute_reply": "2025-07-13T23:28:46.061893Z"
    },
    "papermill": {
     "duration": 24.865282,
     "end_time": "2025-07-13T23:28:46.063847",
     "exception": false,
     "start_time": "2025-07-13T23:28:21.198565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "X = mnist['data']       # Shape: (70000, 784)\n",
    "y = mnist['target']     # Shape: (70000,)\n",
    "\n",
    "X = X / 255.0               # Normalize pixel values to [0, 1]\n",
    "X = (X - 0.1307) / 0.3081   # Standardize\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "X = X.reshape(-1, 1, 28, 28) # Reshape for CNN\n",
    "\n",
    "# Split into train/test (60k train, 10k test)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188f19b",
   "metadata": {
    "papermill": {
     "duration": 0.00188,
     "end_time": "2025-07-13T23:28:46.068006",
     "exception": false,
     "start_time": "2025-07-13T23:28:46.066126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PyTorch Tensors & DataLoaders\n",
    "Convert data to torch tensors and wrap in DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c217e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T23:28:46.072908Z",
     "iopub.status.busy": "2025-07-13T23:28:46.072324Z",
     "iopub.status.idle": "2025-07-13T23:28:46.227974Z",
     "shell.execute_reply": "2025-07-13T23:28:46.227284Z"
    },
    "papermill": {
     "duration": 0.159696,
     "end_time": "2025-07-13T23:28:46.229568",
     "exception": false,
     "start_time": "2025-07-13T23:28:46.069872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167d4db",
   "metadata": {
    "papermill": {
     "duration": 0.001799,
     "end_time": "2025-07-13T23:28:46.233627",
     "exception": false,
     "start_time": "2025-07-13T23:28:46.231828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define LeNet-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe08273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T23:28:46.238371Z",
     "iopub.status.busy": "2025-07-13T23:28:46.238128Z",
     "iopub.status.idle": "2025-07-13T23:28:46.243210Z",
     "shell.execute_reply": "2025-07-13T23:28:46.242658Z"
    },
    "papermill": {
     "duration": 0.008845,
     "end_time": "2025-07-13T23:28:46.244334",
     "exception": false,
     "start_time": "2025-07-13T23:28:46.235489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8f4aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T23:28:46.248799Z",
     "iopub.status.busy": "2025-07-13T23:28:46.248622Z",
     "iopub.status.idle": "2025-07-13T23:34:10.939226Z",
     "shell.execute_reply": "2025-07-13T23:34:10.938518Z"
    },
    "papermill": {
     "duration": 324.697235,
     "end_time": "2025-07-13T23:34:10.943460",
     "exception": false,
     "start_time": "2025-07-13T23:28:46.246225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4126, Train Acc: 0.8905, Val Loss: 0.2272, Val Acc: 0.9383\n",
      "Epoch 2: Train Loss: 0.1985, Train Acc: 0.9455, Val Loss: 0.1602, Val Acc: 0.9552\n",
      "Epoch 3: Train Loss: 0.1473, Train Acc: 0.9591, Val Loss: 0.1240, Val Acc: 0.9657\n",
      "Epoch 4: Train Loss: 0.1180, Train Acc: 0.9668, Val Loss: 0.1031, Val Acc: 0.9702\n",
      "Epoch 5: Train Loss: 0.0999, Train Acc: 0.9722, Val Loss: 0.0892, Val Acc: 0.9743\n",
      "Epoch 6: Train Loss: 0.0882, Train Acc: 0.9753, Val Loss: 0.0808, Val Acc: 0.9755\n",
      "Epoch 7: Train Loss: 0.0794, Train Acc: 0.9776, Val Loss: 0.0733, Val Acc: 0.9773\n",
      "Epoch 8: Train Loss: 0.0731, Train Acc: 0.9799, Val Loss: 0.0684, Val Acc: 0.9794\n",
      "Epoch 9: Train Loss: 0.0677, Train Acc: 0.9813, Val Loss: 0.0616, Val Acc: 0.9812\n",
      "Epoch 10: Train Loss: 0.0634, Train Acc: 0.9826, Val Loss: 0.0593, Val Acc: 0.9819\n",
      "Epoch 11: Train Loss: 0.0597, Train Acc: 0.9839, Val Loss: 0.0564, Val Acc: 0.9829\n",
      "Epoch 12: Train Loss: 0.0566, Train Acc: 0.9843, Val Loss: 0.0538, Val Acc: 0.9830\n",
      "Epoch 13: Train Loss: 0.0537, Train Acc: 0.9852, Val Loss: 0.0515, Val Acc: 0.9837\n",
      "Epoch 14: Train Loss: 0.0513, Train Acc: 0.9859, Val Loss: 0.0497, Val Acc: 0.9844\n",
      "Epoch 15: Train Loss: 0.0492, Train Acc: 0.9863, Val Loss: 0.0479, Val Acc: 0.9850\n",
      "Epoch 16: Train Loss: 0.0472, Train Acc: 0.9870, Val Loss: 0.0459, Val Acc: 0.9847\n",
      "Epoch 17: Train Loss: 0.0456, Train Acc: 0.9873, Val Loss: 0.0456, Val Acc: 0.9853\n",
      "Epoch 18: Train Loss: 0.0441, Train Acc: 0.9879, Val Loss: 0.0434, Val Acc: 0.9867\n",
      "Epoch 19: Train Loss: 0.0426, Train Acc: 0.9883, Val Loss: 0.0436, Val Acc: 0.9862\n",
      "Epoch 20: Train Loss: 0.0412, Train Acc: 0.9885, Val Loss: 0.0422, Val Acc: 0.9858\n",
      "Epoch 21: Train Loss: 0.0400, Train Acc: 0.9892, Val Loss: 0.0409, Val Acc: 0.9880\n",
      "Epoch 22: Train Loss: 0.0387, Train Acc: 0.9896, Val Loss: 0.0406, Val Acc: 0.9870\n",
      "Epoch 23: Train Loss: 0.0378, Train Acc: 0.9894, Val Loss: 0.0400, Val Acc: 0.9871\n",
      "Epoch 24: Train Loss: 0.0368, Train Acc: 0.9898, Val Loss: 0.0397, Val Acc: 0.9875\n",
      "Epoch 25: Train Loss: 0.0358, Train Acc: 0.9904, Val Loss: 0.0385, Val Acc: 0.9875\n",
      "Epoch 26: Train Loss: 0.0348, Train Acc: 0.9907, Val Loss: 0.0382, Val Acc: 0.9875\n",
      "Epoch 27: Train Loss: 0.0341, Train Acc: 0.9908, Val Loss: 0.0380, Val Acc: 0.9880\n",
      "Epoch 28: Train Loss: 0.0332, Train Acc: 0.9909, Val Loss: 0.0370, Val Acc: 0.9882\n",
      "Epoch 29: Train Loss: 0.0325, Train Acc: 0.9911, Val Loss: 0.0372, Val Acc: 0.9878\n",
      "Epoch 30: Train Loss: 0.0318, Train Acc: 0.9916, Val Loss: 0.0356, Val Acc: 0.9880\n",
      "Epoch 31: Train Loss: 0.0311, Train Acc: 0.9918, Val Loss: 0.0351, Val Acc: 0.9886\n",
      "Epoch 32: Train Loss: 0.0305, Train Acc: 0.9918, Val Loss: 0.0362, Val Acc: 0.9873\n",
      "Epoch 33: Train Loss: 0.0297, Train Acc: 0.9921, Val Loss: 0.0355, Val Acc: 0.9890\n",
      "Epoch 34: Train Loss: 0.0292, Train Acc: 0.9923, Val Loss: 0.0349, Val Acc: 0.9890\n",
      "Epoch 35: Train Loss: 0.0286, Train Acc: 0.9924, Val Loss: 0.0345, Val Acc: 0.9889\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "\n",
    "# Apply LeCun and scaled Xavier initialization to Conv2d and Linear layers.\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        fan_in = m.in_channels * m.kernel_size[0] * m.kernel_size[1]\n",
    "        limit = (3.0 / fan_in) ** 0.5\n",
    "        nn.init.uniform_(m.weight, -limit, limit)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        fan_in, fan_out = m.in_features, m.out_features\n",
    "        limit = 4.0 * (6.0 / (fan_in + fan_out)) ** 0.5\n",
    "        nn.init.uniform_(m.weight, -limit, limit)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "# Use DataParallel for multi GPU support (Kaggles GPU T4 x2)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 35\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for xb, yb in train_dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(xb)\n",
    "        loss = loss_fn(yhat, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = torch.argmax(yhat, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(yhat, dim=1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            val_total += xb.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 367.199634,
   "end_time": "2025-07-13T23:34:13.924675",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-13T23:28:06.725041",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
