{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955f14ab",
   "metadata": {
    "papermill": {
     "duration": 0.006331,
     "end_time": "2025-07-13T10:15:16.197119",
     "exception": false,
     "start_time": "2025-07-13T10:15:16.190788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LeNet-5 (1998) – NumPy Implementation from Scratch\n",
    "\n",
    "In this notebook I build LeNet-5, the CNN introduced by\n",
    "Yann LeCun for handwritten digit recognition.\n",
    "\n",
    "* **Dataset** : MNIST (60 k train / 10 k test)  \n",
    "* **Input size** : LeNet expects 32 × 32 greyscale images.  \n",
    "  MNIST is 28 × 28, so we pad 2 pixels on each side in the first\n",
    "  convolution layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaea367",
   "metadata": {
    "papermill": {
     "duration": 0.004536,
     "end_time": "2025-07-13T10:15:16.206705",
     "exception": false,
     "start_time": "2025-07-13T10:15:16.202169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd7a5f",
   "metadata": {
    "papermill": {
     "duration": 0.004425,
     "end_time": "2025-07-13T10:15:16.216293",
     "exception": false,
     "start_time": "2025-07-13T10:15:16.211868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Layer      | Type                 | Parameters                             | Output Shape (input 28x28) |\n",
    "| ---------- | -------------------- | -------------------------------------- | -------------------------- |\n",
    "| **C1**     | Convolution          | 6 filters, 5×5 kernel, stride=1, pad=2 | (6, 28, 28)                |\n",
    "|            | Activation (Tanh) |                                        | (6, 28, 28)                |\n",
    "| **S2**     | Average Pooling      | 2×2 window, stride=2                   | (6, 14, 14)                |\n",
    "| **C3**     | Convolution          | 16 filters, 5×5 kernel, stride=1       | (16, 10, 10)               |\n",
    "|            | Activation (Tanh) |                                        | (16, 10, 10)               |\n",
    "| **S4**     | Average Pooling      | 2×2 window, stride=2                   | (16, 5, 5)                 |\n",
    "| **C5**     | Convolution          | 120 filters, 5×5 kernel, stride=1      | (120, 1, 1)                |\n",
    "|            | Activation (Tanh) |                                        | (120,)                     |\n",
    "| **F6**     | Fully Connected      | 120 → 84                               | (84,)                      |\n",
    "|            | Activation (Sigmoid) |                                        | (84,)                      |\n",
    "| **Output** | Fully Connected      | 84 → 10                                | (10,)                      |\n",
    "\n",
    "![Architecture](figures/Architecture.png)\n",
    "\n",
    "Image source: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. - https://github.com/d2l-ai/d2l-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112d6ea",
   "metadata": {
    "papermill": {
     "duration": 0.00437,
     "end_time": "2025-07-13T10:15:16.225372",
     "exception": false,
     "start_time": "2025-07-13T10:15:16.221002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1  Load and preprocess MNIST\n",
    "Load the MNIST dataset from sklearn, normalize it to [0,1], and reshape for convolution layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0c0b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:16.236381Z",
     "iopub.status.busy": "2025-07-13T10:15:16.236001Z",
     "iopub.status.idle": "2025-07-13T10:15:46.315662Z",
     "shell.execute_reply": "2025-07-13T10:15:46.314587Z"
    },
    "papermill": {
     "duration": 30.086992,
     "end_time": "2025-07-13T10:15:46.317493",
     "exception": false,
     "start_time": "2025-07-13T10:15:16.230501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "X = mnist['data']       # Shape: (70000, 784)\n",
    "y = mnist['target']     # Shape: (70000,)\n",
    "\n",
    "X = X / 255.0           # Normalize pixel values to [0, 1]\n",
    "y = y.astype(np.int32)  # Convert labels to integers\n",
    "\n",
    "X = X.reshape(-1, 1, 28, 28) # Reshape for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61373613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.328860Z",
     "iopub.status.busy": "2025-07-13T10:15:46.328398Z",
     "iopub.status.idle": "2025-07-13T10:15:46.334833Z",
     "shell.execute_reply": "2025-07-13T10:15:46.333808Z"
    },
    "papermill": {
     "duration": 0.014014,
     "end_time": "2025-07-13T10:15:46.336511",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.322497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10000, 1, 28, 28)\n",
      "X_test shape: (10000, 1, 28, 28)\n",
      "y_train shape: (10000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test (10k train, 10k test)\n",
    "X_train, X_test = X[:10000], X[10000:20000]\n",
    "y_train, y_test = y[:10000], y[10000:20000]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a8f74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.348351Z",
     "iopub.status.busy": "2025-07-13T10:15:46.347125Z",
     "iopub.status.idle": "2025-07-13T10:15:46.354199Z",
     "shell.execute_reply": "2025-07-13T10:15:46.353416Z"
    },
    "papermill": {
     "duration": 0.014448,
     "end_time": "2025-07-13T10:15:46.355881",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.341433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 6, 9, 7], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4df10",
   "metadata": {
    "papermill": {
     "duration": 0.004529,
     "end_time": "2025-07-13T10:15:46.366474",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.361945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2  Utility functions\n",
    "Activation, and loss. Documentation is generated by ChatGPT 4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60204c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.377634Z",
     "iopub.status.busy": "2025-07-13T10:15:46.376945Z",
     "iopub.status.idle": "2025-07-13T10:15:46.384953Z",
     "shell.execute_reply": "2025-07-13T10:15:46.383793Z"
    },
    "papermill": {
     "duration": 0.015469,
     "end_time": "2025-07-13T10:15:46.386642",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.371173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Element-wise Sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: np.ndarray or float\n",
    "        Input value(s).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray or float\n",
    "        Output after applying the sigmoid function element-wise.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: np.ndarray or float\n",
    "        Input value(s).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray or float\n",
    "        The derivative of the sigmoid function evaluated at x.\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Element-wise hyperbolic tangent (tanh) activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: np.ndarray or float\n",
    "        Input value(s).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray or float\n",
    "        Output after applying the tanh function element-wise.\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of the hyperbolic tangent (tanh) function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: np.ndarray or float\n",
    "        Input value(s).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray or float\n",
    "        The derivative of tanh, computed as 1 - tanh(x)^2, evaluated at x.\n",
    "    \"\"\"\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities over classes.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: np.ndarray\n",
    "        Logits of shape (batch_size, num_classes).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray\n",
    "        Softmax probabilities of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Mean cross-entropy loss for multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "    - yhat: np.ndarray\n",
    "        Predicted probabilities (batch_size, num_classes).\n",
    "    - y: np.ndarray\n",
    "        One-hot encoded true labels (batch_size, num_classes).\n",
    "    - eps: float, optional\n",
    "        Small epsilon to avoid log(0).\n",
    "\n",
    "    Returns:\n",
    "    - float\n",
    "        Mean cross-entropy loss over the batch.\n",
    "    \"\"\"\n",
    "    yhat = np.clip(yhat, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(y * np.log(yhat), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732807d4",
   "metadata": {
    "papermill": {
     "duration": 0.004559,
     "end_time": "2025-07-13T10:15:46.396135",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.391576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Zero Padding\n",
    "\n",
    "Pad the input to get the 32×32 images LeNet-5 was designed for.  \n",
    "This function does zero-padding around the spatial dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c23df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.407067Z",
     "iopub.status.busy": "2025-07-13T10:15:46.406753Z",
     "iopub.status.idle": "2025-07-13T10:15:46.412065Z",
     "shell.execute_reply": "2025-07-13T10:15:46.411131Z"
    },
    "papermill": {
     "duration": 0.012512,
     "end_time": "2025-07-13T10:15:46.413434",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.400922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding(X: np.ndarray, p: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pads images with zeros.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input tensor of shape (batch, channels, height, width)\n",
    "    - p: Padding size\n",
    "\n",
    "    Returns:\n",
    "    - Zero-padded tensor of shape (batch, channels, height + 2*p, width + 2*p)\n",
    "    \"\"\"\n",
    "    batch, channel, height, width = X.shape\n",
    "    Y = np.zeros((batch, channel, height + 2*p, width + 2*p))\n",
    "    Y[:, :, p:p+height, p:p+width] = X\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef579d70",
   "metadata": {
    "papermill": {
     "duration": 0.004554,
     "end_time": "2025-07-13T10:15:46.422936",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.418382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolution Layer (Forward)\n",
    "\n",
    "This function performs a multi-channel, multi-filter convolution, padding inputs as needed.\n",
    "We implement the forward pass using for loops, summing over input channels and applying each filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52be49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.434049Z",
     "iopub.status.busy": "2025-07-13T10:15:46.433716Z",
     "iopub.status.idle": "2025-07-13T10:15:46.441180Z",
     "shell.execute_reply": "2025-07-13T10:15:46.440334Z"
    },
    "papermill": {
     "duration": 0.014961,
     "end_time": "2025-07-13T10:15:46.442732",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.427771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Convolution(X, p, in_channels, out_channels, K, bias):\n",
    "    \"\"\"\n",
    "    Performs a convolution over multi-channel input with multiple output channels.\n",
    "    \n",
    "    Arguments:\n",
    "    - X: input tensor of shape (batch, in_channels, height, width)\n",
    "    - padding: number of padding pixels on each side\n",
    "    - in_channels: number of input channels\n",
    "    - out_channels: number of output channels (number of filters)\n",
    "    - K: Kernel\n",
    "    - bias: Bias term added to each output channel\n",
    "    \n",
    "    Returns:\n",
    "    - Y: output tensor of shape (batch, out_channels, output_height, output_width)\n",
    "    - X: Padded input for backpropagation\n",
    "    - K: The kernel to be reused\n",
    "    - b: The bias to be reused\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad input\n",
    "    if p > 0:\n",
    "        X = padding(X, p)\n",
    "\n",
    "    batch, channel, height, width = X.shape\n",
    "\n",
    "    # Compute output size (assuming stride=1)\n",
    "    output_height = height - K.shape[2] + 1\n",
    "    output_width  = width - K.shape[3] + 1\n",
    "    Y = np.zeros(shape=(batch, out_channels, output_height, output_width))\n",
    "\n",
    "    \"\"\"\n",
    "    First attempt. Worked but was really slow. Kept here to showcase my struggles.\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            for in_ch in range(in_channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        Y[b, out_ch, h, w] += np.sum(\n",
    "                            X[b, in_ch, h: h + K.shape[2], w: w + K.shape[3]] * \n",
    "                            K[out_ch, in_ch]\n",
    "            Y[b, out_ch, :, :] += bias[out_ch]\n",
    "    \"\"\"\n",
    "    # Vectorized height and width loop.-\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            accum = np.zeros((output_height, output_width))\n",
    "            # Sum over each input channel\n",
    "            for in_ch in range(in_channels):\n",
    "                # This uses a 2D sliding window, no inner h,w loop\n",
    "                for i in range(K.shape[2]):\n",
    "                    for j in range(K.shape[3]):\n",
    "                        accum += X[b, in_ch, i:i+output_height, j:j+output_width] * K[out_ch, in_ch, i, j]\n",
    "            # Add bias\n",
    "            Y[b, out_ch] = accum + bias[out_ch]\n",
    "\n",
    "    return Y, X, K, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00637cc5",
   "metadata": {
    "papermill": {
     "duration": 0.004633,
     "end_time": "2025-07-13T10:15:46.452204",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.447571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolution Backward Pass\n",
    "\n",
    "Computes gradients for weights, biases, and inputs.\n",
    "This is a direct implementation of the chain rule for convolution layers. This was by far the most difficult part of the project and many errors appeared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab397573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.463360Z",
     "iopub.status.busy": "2025-07-13T10:15:46.462999Z",
     "iopub.status.idle": "2025-07-13T10:15:46.471481Z",
     "shell.execute_reply": "2025-07-13T10:15:46.470600Z"
    },
    "papermill": {
     "duration": 0.015852,
     "end_time": "2025-07-13T10:15:46.472982",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.457130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ConvBackward(dY, X, K):\n",
    "    \"\"\"\n",
    "    Backpropagation for a convolution layer.\n",
    "\n",
    "    Computes gradients w.r.t.:\n",
    "    - input X\n",
    "    - kernel weights K\n",
    "    - bias\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient of loss w.r.t. output, shape (batch, out_channels, out_height, out_width)\n",
    "    - X: Input to the convolution, shape (batch, in_channels, height, width)\n",
    "    - K: Kernels, shape (out_channels, in_channels, kH, kW)\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient w.r.t. input X\n",
    "    - dK: Gradient w.r.t. kernels\n",
    "    - db: Gradient w.r.t. biases (averaged over batch)\n",
    "    \"\"\"\n",
    "    batch, in_channels, height, width = X.shape\n",
    "    out_channels, _, kH, kW = K.shape\n",
    "    out_height, out_width = dY.shape[2], dY.shape[3]\n",
    "\n",
    "    dK = np.zeros_like(K)\n",
    "    db = np.sum(dY, axis=(0,2,3)) / batch\n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            for in_ch in range(in_channels):\n",
    "                for i in range(kH):\n",
    "                    for j in range(kW):\n",
    "                        h_end = i + out_height\n",
    "                        w_end = j + out_width\n",
    "                        # clip if we overshoot the input dims\n",
    "                        if h_end > height or w_end > width:\n",
    "                            h_end = min(h_end, height)\n",
    "                            w_end = min(w_end, width)\n",
    "                            slice_h = h_end - i\n",
    "                            slice_w = w_end - j\n",
    "                            # also slice dY to match\n",
    "                            dY_slice = dY[b, out_ch, :slice_h, :slice_w]\n",
    "                            X_slice = X[b, in_ch, i:h_end, j:w_end]\n",
    "                        else:\n",
    "                            X_slice = X[b, in_ch, i:h_end, j:w_end]\n",
    "                            dY_slice = dY[b, out_ch]\n",
    "\n",
    "                        # Accumulate gradients for kernel weights\n",
    "                        dK[out_ch, in_ch, i, j] += np.sum(X_slice * dY_slice) / batch\n",
    "\n",
    "                         # Accumulate gradients for input\n",
    "                        dX[b, in_ch, i:h_end, j:w_end] += K[out_ch, in_ch, i, j] * dY_slice\n",
    "    return dX, dK, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c4e9b",
   "metadata": {
    "papermill": {
     "duration": 0.004542,
     "end_time": "2025-07-13T10:15:46.482904",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.478362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Average Pooling Layer (Forward)\n",
    "\n",
    "Implements average pooling with a specified kernel size and stride.\n",
    "This uses a vectorized sliding window summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640ff67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.494945Z",
     "iopub.status.busy": "2025-07-13T10:15:46.494105Z",
     "iopub.status.idle": "2025-07-13T10:15:46.501248Z",
     "shell.execute_reply": "2025-07-13T10:15:46.500427Z"
    },
    "papermill": {
     "duration": 0.014794,
     "end_time": "2025-07-13T10:15:46.502971",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.488177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Pooling(X, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Average pooling operation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input tensor of shape (batch, channel, height, width)\n",
    "    - kernel_size: Tuple (kH, kW) specifying pooling window size\n",
    "    - stride: Stride of the pooling operation\n",
    "\n",
    "    Returns:\n",
    "    - Y: Output tensor after pooling\n",
    "    \"\"\"\n",
    "\n",
    "    batch, channel, height, width = X.shape\n",
    "    output_height = (height - kernel_size[0]) // stride + 1\n",
    "    output_width  = (width - kernel_size[1]) // stride + 1\n",
    "    Y = np.zeros(shape=(batch, channel, output_height, output_width))\n",
    "\n",
    "    \"\"\"\n",
    "    First attempt, too slow.\n",
    "    for b in range(batch):\n",
    "        for ch in range(channel):\n",
    "            for h in range(output_height):\n",
    "                for w in range(output_width):\n",
    "                    # Calculated this from a piece of paper, helped me understand everything a lot better.\n",
    "                    Y[b, ch, h, w] = np.mean(X[b, ch, \n",
    "                                               (h * stride): (h * stride) + kernel_size[0],\n",
    "                                               (w * stride): (w * stride) + kernel_size[1]])\n",
    "    \"\"\"\n",
    "\n",
    "    window_sum = np.zeros((batch, channel, output_height, output_width))\n",
    "    for i in range(kernel_size[0]):\n",
    "        for j in range(kernel_size[1]):\n",
    "            window_sum += X[:, :, i:i+output_height*stride:stride, j:j+output_width*stride:stride]\n",
    "    Y = window_sum / (kernel_size[0] * kernel_size[1])\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65453c31",
   "metadata": {
    "papermill": {
     "duration": 0.004765,
     "end_time": "2025-07-13T10:15:46.512717",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.507952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Average Pooling Backward Pass\n",
    "\n",
    "Distributes gradients equally over the region covered by each pooling window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ad7fb65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.523935Z",
     "iopub.status.busy": "2025-07-13T10:15:46.523575Z",
     "iopub.status.idle": "2025-07-13T10:15:46.529804Z",
     "shell.execute_reply": "2025-07-13T10:15:46.528951Z"
    },
    "papermill": {
     "duration": 0.013564,
     "end_time": "2025-07-13T10:15:46.531193",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.517629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PoolingBackward(dY, X, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Backpropagation for average pooling.\n",
    "\n",
    "    Spreads gradient uniformly over the region that was pooled.\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient w.r.t. output, shape (batch, channel, out_height, out_width)\n",
    "    - X: Input to pooling layer, shape (batch, channel, height, width)\n",
    "    - kernel_size: Tuple (kH, kW)\n",
    "    - stride: Pooling stride\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient w.r.t. input X\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, channel, height, width = X.shape\n",
    "    output_height, output_width = dY.shape[2], dY.shape[3]\n",
    "    kH, kW = kernel_size\n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for b in range(batch):\n",
    "        for ch in range(channel):\n",
    "            for h in range(output_height):\n",
    "                for w in range(output_width):\n",
    "                    dX[b, ch,\n",
    "                       (h * stride):(h * stride) + kH,\n",
    "                       (w * stride):(w * stride) + kW] += dY[b, ch, h, w] / (kH * kW)\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15194",
   "metadata": {
    "papermill": {
     "duration": 0.004709,
     "end_time": "2025-07-13T10:15:46.540865",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.536156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weight Initialization\n",
    "\n",
    "We initialize weights differently depending on the activation function and layer type\n",
    "to help maintain stable variance and gradients during training:\n",
    "\n",
    "- **Convolutional layers (tanh activations)**:\n",
    "  Use LeCun uniform initialization, which samples weights from\n",
    "  Uniform(-sqrt(3 / fan_in), sqrt(3 / fan_in)).\n",
    "  This preserves the variance of activations when using tanh.\n",
    "\n",
    "- **Fully connected layers (sigmoid activations)**:\n",
    "  Use Xavier initialization scaled for sigmoid, sampling from\n",
    "  Uniform(-4 * sqrt(6 / (fan_in + fan_out)), 4 * sqrt(6 / (fan_in + fan_out))).\n",
    "  The scale factor (4) compensates for the maximum slope of sigmoid (~0.25).\n",
    "\n",
    "- **Biases** are initialized to zeros for all layers.\n",
    "\n",
    "This initialization reduces the risk of vanishing or exploding activations, which this model suffered from when using default Xavier Initialization on Sigmoid. Gradients vanished really fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2795b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.551776Z",
     "iopub.status.busy": "2025-07-13T10:15:46.551462Z",
     "iopub.status.idle": "2025-07-13T10:15:46.556789Z",
     "shell.execute_reply": "2025-07-13T10:15:46.555537Z"
    },
    "papermill": {
     "duration": 0.012551,
     "end_time": "2025-07-13T10:15:46.558367",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.545816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lecun_tanh_init(size, n_in):\n",
    "    \"\"\"\n",
    "    LeCun uniform initialization for weights used with tanh activations.\n",
    "\n",
    "    Parameters:\n",
    "    - size : tuple\n",
    "        Shape of the weight tensor (e.g., (out_channels, in_channels, kernel_h, kernel_w)).\n",
    "    - n_in : int\n",
    "        Number of input units (fan_in). For convolution, this is typically\n",
    "        kernel_height * kernel_width * input_channels.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray\n",
    "        Initialized weights sampled from Uniform(-limit, limit),\n",
    "        where limit = sqrt(3 / n_in).\n",
    "        This preserves variance for layers using tanh activations.\n",
    "    \"\"\"\n",
    "    limit = np.sqrt(3.0 / n_in)\n",
    "    return np.random.uniform(-limit, limit, size=size)\n",
    "\n",
    "\n",
    "def xavier_sigmoid_init(size, n_in, n_out):\n",
    "    \"\"\"\n",
    "    Xavier (Glorot) initialization scaled for sigmoid activations.\n",
    "\n",
    "    Since sigmoid’s max slope is ~0.25, we use:\n",
    "    limit = 4 * sqrt(6 / (fan_in + fan_out))\n",
    "\n",
    "    Parameters:\n",
    "    - size : tuple\n",
    "        Shape of the weight matrix (e.g., (output_units, input_units)).\n",
    "    - n_in : int\n",
    "        Number of input units (fan_in).\n",
    "    - n_out : int\n",
    "        Number of output units (fan_out).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray\n",
    "        Initialized weights sampled from Uniform(-limit, limit).\n",
    "    \"\"\"\n",
    "    limit = 4.0 * np.sqrt(6.0 / (n_in + n_out))\n",
    "    return np.random.uniform(-limit, limit, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c5a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.586901Z",
     "iopub.status.busy": "2025-07-13T10:15:46.586156Z",
     "iopub.status.idle": "2025-07-13T10:15:46.594375Z",
     "shell.execute_reply": "2025-07-13T10:15:46.593440Z"
    },
    "papermill": {
     "duration": 0.015332,
     "end_time": "2025-07-13T10:15:46.595928",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.580596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LeCun-tanh initialization for each convolutional layer\n",
    "\n",
    "# C1: 6 filters, 1 input channel, 5x5 kernel\n",
    "c1_kernel = lecun_tanh_init(size=(6, 1, 5, 5), n_in=25)\n",
    "c1_bias = np.zeros(6)\n",
    "\n",
    "# C3: 16 filters, 6 input channels\n",
    "c3_kernel = lecun_tanh_init(size=(16, 6, 5, 5), n_in=150)\n",
    "c3_bias = np.zeros(16)\n",
    "\n",
    "# C5: 120 filters, 16 input channels\n",
    "c5_kernel = lecun_tanh_init(size=(120, 16, 5, 5), n_in=400)\n",
    "c5_bias = np.zeros(120)\n",
    "\n",
    "# Xavier initialization for each fc layer\n",
    "\n",
    "# F6: fully connected 120 -> 84\n",
    "f6_weights = xavier_sigmoid_init((84, 120), n_in=120, n_out=84)\n",
    "f6_bias = np.zeros(84)\n",
    "\n",
    "# Output layer: fully connected 84 -> 10\n",
    "out_weights = xavier_sigmoid_init((10, 84), n_in=84, n_out=10)\n",
    "out_bias = np.zeros(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07874023",
   "metadata": {
    "papermill": {
     "duration": 0.00444,
     "end_time": "2025-07-13T10:15:46.605220",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.600780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Loop\n",
    "\n",
    "Trains the network over multiple epochs. Uses SGD to update weights.\n",
    "Includes explicit forward and backward passes through all layers.\n",
    "Training on only 4 epochs on 10000 images because it is very slow.\n",
    "\n",
    "We also shuffle the data each epoch to improve convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a67b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:15:46.615973Z",
     "iopub.status.busy": "2025-07-13T10:15:46.615664Z",
     "iopub.status.idle": "2025-07-13T18:53:02.779977Z",
     "shell.execute_reply": "2025-07-13T18:53:02.778339Z"
    },
    "papermill": {
     "duration": 31036.175042,
     "end_time": "2025-07-13T18:53:02.785016",
     "exception": false,
     "start_time": "2025-07-13T10:15:46.609974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.6697 | Accuracy: 0.8205\n",
      "Epoch 2 | Loss: 0.2234 | Accuracy: 0.9364\n",
      "Epoch 3 | Loss: 0.1726 | Accuracy: 0.9509\n",
      "Epoch 4 | Loss: 0.1387 | Accuracy: 0.9627\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 4\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0    # Sum of losses for this epoch\n",
    "    correct = 0       # Count of correct predictions\n",
    "\n",
    "    # Shuffle data each epoch\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    \n",
    "    for batch in range(0, X_train.shape[0], batch_size):\n",
    "        end = min(batch + batch_size, X_train.shape[0])\n",
    "        X_train_batch = X_train[batch:end]\n",
    "        y_train_batch = y_train[batch:end]\n",
    "        batch_len = X_train_batch.shape[0]\n",
    "    \n",
    "        # Forward Pass\n",
    "        c1, X1, c1_kernel, c1_bias = Convolution(X_train_batch,\n",
    "                                                p=2, \n",
    "                                                in_channels=1, \n",
    "                                                out_channels=6,\n",
    "                                                K=c1_kernel,\n",
    "                                                bias=c1_bias)\n",
    "        a1 = tanh(c1)\n",
    "        s2 = Pooling(a1, kernel_size=(2, 2), stride=2)\n",
    "        c3, _, c3_kernel, c3_bias = Convolution(s2,\n",
    "                                                p=0,\n",
    "                                                in_channels=6,\n",
    "                                                out_channels=16,\n",
    "                                                K=c3_kernel,\n",
    "                                                bias=c3_bias\n",
    "                                                )\n",
    "        a3 = tanh(c3)\n",
    "        s4 = Pooling(a3, kernel_size=(2, 2), stride=2)\n",
    "        c5, _, c5_kernel, c5_bias = Convolution(s4,\n",
    "                                                p=0,\n",
    "                                                in_channels=16,\n",
    "                                                out_channels=120,\n",
    "                                                K=c5_kernel,\n",
    "                                                bias=c5_bias\n",
    "                                                )\n",
    "        a5 = tanh(c5).reshape(batch_len, 120)\n",
    "        f6 = np.dot(a5, f6_weights.T) + f6_bias\n",
    "        a6 = sigmoid(f6)\n",
    "        output = np.dot(a6, out_weights.T) + out_bias\n",
    "        yhat = softmax(output)\n",
    "\n",
    "        # Onehot Encoding Labels\n",
    "        y_onehot = np.zeros((batch_len, 10))\n",
    "        y_onehot[np.arange(batch_len), y_train_batch] = 1   # Turn class indices into one-hot vectors\n",
    "\n",
    "        # Loss and Accuracy\n",
    "        loss = CrossEntropy(yhat, y_onehot)                 # Average loss over batch\n",
    "        total_loss += loss * batch_len                      # Not multiplying by batch_len was an error\n",
    "        preds = np.argmax(yhat, axis=1)                     # Class prediction per sample\n",
    "        correct += np.sum(preds == y_train_batch)           # Tally correct predictions\n",
    "\n",
    "        dz_output = yhat - y_onehot\n",
    "        # Grad for output layer\n",
    "        dw_out = np.dot(dz_output.T, a6) / batch_len  # shape (10, 84)\n",
    "        db_out = dz_output.sum(axis=0) / batch_len    # shape (10,)\n",
    "\n",
    "        # Propagate to hidden layer\n",
    "        da6 = np.dot(dz_output, out_weights)          # (batch, 84)\n",
    "        dz6 = da6 * sigmoid_deriv(f6)                 # (batch, 84)\n",
    "\n",
    "        # Grad for F6 layer\n",
    "        dw_f6 = np.dot(dz6.T, a5) / batch_len         # shape (84, 120)\n",
    "        db_f6 = dz6.sum(axis=0) / batch_len           # shape (84,)\n",
    "\n",
    "        # Propagate to C5 output\n",
    "        da5 = np.dot(dz6, f6_weights)                 # (batch, 120)\n",
    "        da5 = da5.reshape(batch_len, 120, 1, 1)       # for compatibility with conv\n",
    "\n",
    "        # Propagate through C5 layer\n",
    "        dz5 = da5 * tanh_deriv(c5)\n",
    "        ds4, dK5, db5 = ConvBackward(dz5, s4, c5_kernel)\n",
    "\n",
    "        # Propagate through S4\n",
    "        da3 = PoolingBackward(ds4, a3, (2,2), 2)\n",
    "\n",
    "        #Propagate through C3\n",
    "        ds2, dK3, db3 = ConvBackward(da3 * tanh_deriv(c3), s2, c3_kernel)\n",
    "\n",
    "        #Propagate through S2\n",
    "        da1 = PoolingBackward(ds2, a1, (2,2), 2)\n",
    "\n",
    "        #Propagate through C1\n",
    "        _, dK1, db1 = ConvBackward(da1 * tanh_deriv(c1), X1, c1_kernel)\n",
    "\n",
    "        \"\"\"\n",
    "        def grad_stats(name, grad):\n",
    "            abs_grad = np.abs(grad)\n",
    "            print(f\"{name:<8} | mean: {abs_grad.mean():.3e}  std: {abs_grad.std():.3e}  min: {abs_grad.min():.3e}  max: {abs_grad.max():.3e}\")\n",
    "        \n",
    "        grad_stats(\"dK5\", dK5)\n",
    "        grad_stats(\"da3\", da3)\n",
    "        grad_stats(\"dK3\", dK3)\n",
    "        grad_stats(\"dK1\", dK1)\n",
    "        grad_stats(\"dw_f6\", dw_f6)\n",
    "        grad_stats(\"dw_out\", dw_out)\n",
    "\n",
    "        Debugging because accuracy was stagnant at 10% accuracy. Gradients on average\n",
    "        extremely small. I was using Xavier Initialization but then switched.\n",
    "        \"\"\" \n",
    "\n",
    "        out_weights -= lr * dw_out\n",
    "        out_bias -= lr * db_out\n",
    "        f6_weights -= lr * dw_f6\n",
    "        f6_bias -= lr * db_f6\n",
    "        c5_kernel -= lr * dK5\n",
    "        c5_bias -= lr * db5\n",
    "        c3_kernel -= lr * dK3\n",
    "        c3_bias -= lr * db3\n",
    "        c1_kernel -= lr * dK1\n",
    "        c1_bias -= lr * db1\n",
    "\n",
    "    # Epoch Results\n",
    "    acc = correct / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / X_train.shape[0]:.4f} | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc19359",
   "metadata": {},
   "source": [
    "Saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a163e352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:53:02.798624Z",
     "iopub.status.busy": "2025-07-13T18:53:02.797594Z",
     "iopub.status.idle": "2025-07-13T18:53:02.808661Z",
     "shell.execute_reply": "2025-07-13T18:53:02.807614Z"
    },
    "papermill": {
     "duration": 0.020284,
     "end_time": "2025-07-13T18:53:02.810369",
     "exception": false,
     "start_time": "2025-07-13T18:53:02.790085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(\"cnn_params.npz\",\n",
    "         c1_kernel=c1_kernel, c1_bias=c1_bias,\n",
    "         c3_kernel=c3_kernel, c3_bias=c3_bias,\n",
    "         c5_kernel=c5_kernel, c5_bias=c5_bias,\n",
    "         f6_weights=f6_weights, f6_bias=f6_bias,\n",
    "         out_weights=out_weights, out_bias=out_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee28bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load(\"cnn_params.npz\")\n",
    "c1_kernel = params[\"c1_kernel\"]\n",
    "c1_bias = params[\"c1_bias\"]\n",
    "c3_kernel = params[\"c3_kernel\"]\n",
    "c3_bias = params[\"c3_bias\"]\n",
    "c5_kernel = params[\"c5_kernel\"]\n",
    "c5_bias = params[\"c5_bias\"]\n",
    "f6_weights = params[\"f6_weights\"]\n",
    "f6_bias = params[\"f6_bias\"]\n",
    "out_weights = params[\"out_weights\"]\n",
    "out_bias = params[\"out_bias\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce34ac",
   "metadata": {},
   "source": [
    "Running the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ba6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9384\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch in range(0, X_test.shape[0], batch_size):\n",
    "    end = min(batch + batch_size, X_test.shape[0])\n",
    "    X_test_batch = X_test[batch:end]\n",
    "    y_test_batch = y_test[batch:end]\n",
    "    batch_len = X_test_batch.shape[0]\n",
    "\n",
    "    c1, _, _, _ = Convolution(X_test_batch,\n",
    "                              p=2, \n",
    "                              in_channels=1, \n",
    "                              out_channels=6,\n",
    "                              K=c1_kernel,\n",
    "                              bias=c1_bias)\n",
    "    a1 = tanh(c1)\n",
    "    s2 = Pooling(a1, kernel_size=(2, 2), stride=2)\n",
    "    c3, _, _, _ = Convolution(s2,\n",
    "                              p=0,\n",
    "                              in_channels=6,\n",
    "                              out_channels=16,\n",
    "                              K=c3_kernel,\n",
    "                              bias=c3_bias\n",
    "                             )\n",
    "    a3 = tanh(c3)\n",
    "    s4 = Pooling(a3, kernel_size=(2, 2), stride=2)\n",
    "    c5, _, _, _ = Convolution(s4,\n",
    "                              p=0,\n",
    "                              in_channels=16,\n",
    "                              out_channels=120,\n",
    "                              K=c5_kernel,\n",
    "                              bias=c5_bias\n",
    "                             )\n",
    "    a5 = tanh(c5).reshape(batch_len, 120)\n",
    "    f6 = np.dot(a5, f6_weights.T) + f6_bias\n",
    "    a6 = sigmoid(f6)\n",
    "    output = np.dot(a6, out_weights.T) + out_bias\n",
    "    yhat = softmax(output)\n",
    "\n",
    "    preds = np.argmax(yhat, axis=1)\n",
    "    correct += np.sum(preds == y_test_batch)\n",
    "    total += batch_len\n",
    "\n",
    "acc = correct / total\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ed379",
   "metadata": {
    "papermill": {
     "duration": 0.004827,
     "end_time": "2025-07-13T18:53:02.820422",
     "exception": false,
     "start_time": "2025-07-13T18:53:02.815595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, I successfully implemented the LeNet-5 architecture from scratch in Numpy, including forward and backward passes for convolutions, pooling, and fc layers. It was extremely slow so I trained on only 10,000 images. I then replicated it in PyTorch to train for a longer time. I made two mistakes that took me a long time to debug.\n",
    "\n",
    "Backpropagation through padding:\n",
    "Initially, I forgot to account for the padding applied in the first convolutional layer during backpropagation. This led to incorrect gradient computations because the backward pass was slicing over the original unpadded input, mismatching the forward pass, causing a stagnant 10% accuracy (randomly guessing). I fixed this by making my convolution function return the padded input, then making backpropagation use that padded input instead of the default input.\n",
    "\n",
    "Mismatched activation functions and initialization:\n",
    "After I fixed the padding error, it was still stagnant at 10% accuracy. I printed out the gradients and saw that all of them were tiny, on verge of vanishing. Even if they didn't vanish, updates would be ridiculously slow. I double checked the architecture and saw that I made a mistake. I used sigmoid activations across all layers, including the convolutional ones, with Xavier initialization. I thought LeNet-5 used only sigmoid as the activation function, but it actually used Tanh for the convolutional layers, and sigmoid for the fc layers. I fixed that and used LeCun as initialization for the convolutional layers, and Xavier initialization with a bit of scaling for the fc layers. This dramatically improved gradient flow and allowed the network to actually learn.\n",
    "\n",
    "Ultimately, this project not only resulted in a functional LeNet-5 implementation from scratch but also gave me appreciation for engineering optimizations embedded in libraries like PyTorch. They efficiently handle operation and backpropagation details that, when done manually, is extremely slow. Running the PyTorch version on a GPU shortened the training time from 10s of hours to minutes.\n",
    "\n",
    "This project left me with a much stronger intuition for the data flow, mathematical operations, and design choices inside convolutional neural networks."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31072.562185,
   "end_time": "2025-07-13T18:53:03.677630",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-13T10:15:11.115445",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
