{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52f4d45",
   "metadata": {},
   "source": [
    "# ResNet-34 From Scratch\n",
    "\n",
    "Using Tiny ImageNet, which contains 64×64 images instead of ImageNet’s standard 224×224.  \n",
    "Created a modified architecture by changing the first 7×7 convolution into a 3×3 convolution with padding so the height and width remain the same. Also removed the first MaxPool layer. \n",
    "\n",
    "I used Random cropping with padding, horizontal flipping and normalized with dataset-specific mean and std values computed in mean_std.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca1f84",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T04:11:09.093862Z",
     "iopub.status.busy": "2025-07-09T04:11:09.093583Z",
     "iopub.status.idle": "2025-07-09T04:13:55.584548Z",
     "shell.execute_reply": "2025-07-09T04:13:55.583618Z"
    },
    "papermill": {
     "duration": 166.495767,
     "end_time": "2025-07-09T04:13:55.586300",
     "exception": false,
     "start_time": "2025-07-09T04:11:09.090533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(size=(64,64), padding=4),         # slightly pad and then crop back to 64x64\n",
    "    transforms.RandomHorizontalFlip(),                      # randomly flip images left and right\n",
    "    transforms.ToTensor(),                                  # convert image to tensor\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255]))\n",
    "])\n",
    "\n",
    "data_dir = '/kaggle/input/tiny-imagenet-200/tiny-imagenet-200/train'\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733eb0c6",
   "metadata": {},
   "source": [
    "## Residual Block (BasicBlock)\n",
    "\n",
    "Architecture uses the standard ResNet block:\n",
    "- Two 3×3 convolutions, each followed by BatchNorm and ReLU.\n",
    "- A **skip connection (shortcut)** which either:\n",
    "  - uses an Identity if the input and output shapes are the same, or\n",
    "  - uses a 1×1 convolution with matching stride and channels to align shapes when needed.\n",
    "- The addition of the skip path to the output is followed by a final ReLU activation.\n",
    "\n",
    "\n",
    "If the block changes the spatial resolution or the number of channels, \n",
    "the skip connection uses a 1×1 convolution with matching stride and output channels to adjust dimensions before addition.\n",
    "\n",
    "Image below comes from d2l.ai, it showcases the ResNet block with and without the skip connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d1f31",
   "metadata": {},
   "source": [
    "![Res_block](figures/resnet-block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c50501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T04:13:55.591317Z",
     "iopub.status.busy": "2025-07-09T04:13:55.590944Z",
     "iopub.status.idle": "2025-07-09T04:13:55.602792Z",
     "shell.execute_reply": "2025-07-09T04:13:55.601984Z"
    },
    "papermill": {
     "duration": 0.015382,
     "end_time": "2025-07-09T04:13:55.604054",
     "exception": false,
     "start_time": "2025-07-09T04:13:55.588672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch:int, out_ch:int, stride:int):\n",
    "        super().__init__()\n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "\n",
    "        # Skip connection (identity or projection to match dims)\n",
    "        self.skip = nn.Identity()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            # If spatial size cahnges, adjust using a 1x1 conv\n",
    "            self.skip = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=1, stride=stride)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sequence(x)\n",
    "        x = self.skip(x)\n",
    "        return self.relu(x + out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabf08d",
   "metadata": {},
   "source": [
    "## ResNetStack\n",
    "This builds a stack of multiple residual blocks:\n",
    "- The first block handles any downsampling.\n",
    "- Remaining blocks always have stride 1 to preserve dimensions.\n",
    "\n",
    "Each stage (conv2_x, conv3_x...) is implemented as a ResNetStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7765ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetStack(nn.Module):\n",
    "    def __init__(self, in_ch:int, out_ch:int, stride:int, blocks:int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        # First block handles stride or channel change\n",
    "        layers.append(ResNetBlock(in_ch, out_ch, stride))\n",
    "\n",
    "        # Remaining blocks keep stride=1\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResNetBlock(out_ch, out_ch, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386e7e5",
   "metadata": {},
   "source": [
    "## ResNet-34 Block Overview\n",
    "\n",
    "| Block    | k | s | p | out channels | repeats | downsample  |\n",
    "| -------- | - | - | - | ------------ | ------- | ------------|\n",
    "| conv2\\_x | 3 | 1 | 1 | 64           | 3       | No          |\n",
    "| conv3\\_x | 3 | 2 | 1 | 128          | 4       | Yes         |\n",
    "| conv4\\_x | 3 | 2 | 1 | 256          | 6       | Yes         |\n",
    "| conv5\\_x | 3 | 2 | 1 | 512          | 3       | Yes         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433eeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Changed from 7x7 (stride=2) to 3x3 (stride=1) to keep high resolution for small images (64x64)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # conv2_x: keeps resolution, repeat=3\n",
    "        self.conv2_x = ResNetStack(in_ch=64, out_ch=64, stride=1, blocks=3)\n",
    "        \n",
    "        # conv3_x: first block downsamples (stride=2), then repeat=4\n",
    "        self.conv3_x = ResNetStack(in_ch=64, out_ch=128, stride=2, blocks=4)\n",
    "        \n",
    "        # conv4_x: first block downsamples (stride=2), then repeat=6\n",
    "        self.conv4_x = ResNetStack(in_ch=128, out_ch=256, stride=2, blocks=6)\n",
    "        \n",
    "        # conv5_x: first block downsamples (stride=2), then repeat=3\n",
    "        self.conv5_x = ResNetStack(in_ch=256, out_ch=512, stride=2, blocks=3)\n",
    "\n",
    "        # Global average pooling (outputs 1x1x512)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Fully connected layer to 200 Tiny ImageNet classes\n",
    "        self.linear = nn.Linear(in_features=512, out_features=200)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cc576",
   "metadata": {},
   "source": [
    "| Parameter    | Value                                  |\n",
    "| ------------ | -------------------------------------- |\n",
    "| optimizer    | SGD + momentum=0.9                     |\n",
    "| lr           | 0.1 (step decay or cosine)             |\n",
    "| weight decay | 1e-4                                   |\n",
    "| batch size   | 128                                    |\n",
    "| epochs       | 80-100                                 |\n",
    "| augmentation | RandomCrop(64,4), RandomHorizontalFlip |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de32fd",
   "metadata": {},
   "source": [
    "Using CosineAnnealingLR scheduler, starting at lr=0.01.\n",
    "It gradually decays to nearly 0 by the end of 100 epochs.\n",
    "This often improves final validation accuracy.\n",
    "\n",
    "Training loop runs for 100 epochs, tracking both training and validation loss/accuracy. Saves the best model checkpoint based on validation accuracy. And saves a JSON history of all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49748e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T04:13:55.607979Z",
     "iopub.status.busy": "2025-07-09T04:13:55.607728Z",
     "iopub.status.idle": "2025-07-09T11:14:39.641766Z",
     "shell.execute_reply": "2025-07-09T11:14:39.640781Z"
    },
    "papermill": {
     "duration": 25244.041816,
     "end_time": "2025-07-09T11:14:39.647444",
     "exception": false,
     "start_time": "2025-07-09T04:13:55.605628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Saved new best model.\n",
      "Epoch 1: Train Loss: 4.7311, Train Acc: 0.0477, Val Loss: 4.2720, Val Acc: 0.0943\n",
      "Saved new best model.\n",
      "Epoch 2: Train Loss: 3.9806, Train Acc: 0.1268, Val Loss: 3.8197, Val Acc: 0.1540\n",
      "Saved new best model.\n",
      "Epoch 3: Train Loss: 3.5161, Train Acc: 0.1955, Val Loss: 3.3646, Val Acc: 0.2255\n",
      "Saved new best model.\n",
      "Epoch 4: Train Loss: 3.1915, Train Acc: 0.2562, Val Loss: 3.0194, Val Acc: 0.2880\n",
      "Saved new best model.\n",
      "Epoch 5: Train Loss: 2.9295, Train Acc: 0.3056, Val Loss: 2.9033, Val Acc: 0.3241\n",
      "Saved new best model.\n",
      "Epoch 6: Train Loss: 2.7297, Train Acc: 0.3485, Val Loss: 2.8040, Val Acc: 0.3380\n",
      "Saved new best model.\n",
      "Epoch 7: Train Loss: 2.5605, Train Acc: 0.3794, Val Loss: 2.6095, Val Acc: 0.3713\n",
      "Saved new best model.\n",
      "Epoch 8: Train Loss: 2.4083, Train Acc: 0.4116, Val Loss: 2.4567, Val Acc: 0.4050\n",
      "Saved new best model.\n",
      "Epoch 9: Train Loss: 2.2734, Train Acc: 0.4418, Val Loss: 2.3369, Val Acc: 0.4312\n",
      "Saved new best model.\n",
      "Epoch 10: Train Loss: 2.1626, Train Acc: 0.4659, Val Loss: 2.3008, Val Acc: 0.4388\n",
      "Saved new best model.\n",
      "Epoch 11: Train Loss: 2.0461, Train Acc: 0.4898, Val Loss: 2.2052, Val Acc: 0.4638\n",
      "Saved new best model.\n",
      "Epoch 12: Train Loss: 1.9488, Train Acc: 0.5127, Val Loss: 2.2168, Val Acc: 0.4687\n",
      "Saved new best model.\n",
      "Epoch 13: Train Loss: 1.8691, Train Acc: 0.5284, Val Loss: 2.1233, Val Acc: 0.4807\n",
      "Saved new best model.\n",
      "Epoch 14: Train Loss: 1.7771, Train Acc: 0.5496, Val Loss: 2.1341, Val Acc: 0.4843\n",
      "Saved new best model.\n",
      "Epoch 15: Train Loss: 1.6951, Train Acc: 0.5692, Val Loss: 2.0690, Val Acc: 0.4974\n",
      "Saved new best model.\n",
      "Epoch 16: Train Loss: 1.6262, Train Acc: 0.5849, Val Loss: 2.0813, Val Acc: 0.4990\n",
      "Saved new best model.\n",
      "Epoch 17: Train Loss: 1.5532, Train Acc: 0.6006, Val Loss: 2.0348, Val Acc: 0.5149\n",
      "Epoch 18: Train Loss: 1.4876, Train Acc: 0.6177, Val Loss: 2.0665, Val Acc: 0.5066\n",
      "Saved new best model.\n",
      "Epoch 19: Train Loss: 1.4211, Train Acc: 0.6309, Val Loss: 2.0199, Val Acc: 0.5176\n",
      "Saved new best model.\n",
      "Epoch 20: Train Loss: 1.3642, Train Acc: 0.6458, Val Loss: 2.0350, Val Acc: 0.5179\n",
      "Saved new best model.\n",
      "Epoch 21: Train Loss: 1.3078, Train Acc: 0.6587, Val Loss: 2.0519, Val Acc: 0.5180\n",
      "Saved new best model.\n",
      "Epoch 22: Train Loss: 1.2574, Train Acc: 0.6701, Val Loss: 2.0322, Val Acc: 0.5276\n",
      "Saved new best model.\n",
      "Epoch 23: Train Loss: 1.2021, Train Acc: 0.6828, Val Loss: 1.9974, Val Acc: 0.5339\n",
      "Epoch 24: Train Loss: 1.1504, Train Acc: 0.6960, Val Loss: 2.0537, Val Acc: 0.5240\n",
      "Epoch 25: Train Loss: 1.1015, Train Acc: 0.7065, Val Loss: 2.0445, Val Acc: 0.5264\n",
      "Saved new best model.\n",
      "Epoch 26: Train Loss: 1.0638, Train Acc: 0.7148, Val Loss: 2.0088, Val Acc: 0.5345\n",
      "Epoch 27: Train Loss: 1.0072, Train Acc: 0.7297, Val Loss: 2.1028, Val Acc: 0.5269\n",
      "Epoch 28: Train Loss: 0.9725, Train Acc: 0.7370, Val Loss: 2.0906, Val Acc: 0.5326\n",
      "Epoch 29: Train Loss: 0.9307, Train Acc: 0.7472, Val Loss: 2.1221, Val Acc: 0.5287\n",
      "Saved new best model.\n",
      "Epoch 30: Train Loss: 0.8902, Train Acc: 0.7575, Val Loss: 2.0933, Val Acc: 0.5403\n",
      "Epoch 31: Train Loss: 0.8528, Train Acc: 0.7678, Val Loss: 2.2328, Val Acc: 0.5186\n",
      "Epoch 32: Train Loss: 0.8116, Train Acc: 0.7795, Val Loss: 2.1161, Val Acc: 0.5316\n",
      "Saved new best model.\n",
      "Epoch 33: Train Loss: 0.7603, Train Acc: 0.7898, Val Loss: 2.0843, Val Acc: 0.5461\n",
      "Epoch 34: Train Loss: 0.7327, Train Acc: 0.7984, Val Loss: 2.1445, Val Acc: 0.5292\n",
      "Epoch 35: Train Loss: 0.7022, Train Acc: 0.8058, Val Loss: 2.1327, Val Acc: 0.5327\n",
      "Epoch 36: Train Loss: 0.6702, Train Acc: 0.8132, Val Loss: 2.2380, Val Acc: 0.5258\n",
      "Epoch 37: Train Loss: 0.6399, Train Acc: 0.8226, Val Loss: 2.1603, Val Acc: 0.5322\n",
      "Epoch 38: Train Loss: 0.6096, Train Acc: 0.8301, Val Loss: 2.1567, Val Acc: 0.5349\n",
      "Epoch 39: Train Loss: 0.5688, Train Acc: 0.8413, Val Loss: 2.1884, Val Acc: 0.5350\n",
      "Epoch 40: Train Loss: 0.5418, Train Acc: 0.8480, Val Loss: 2.1874, Val Acc: 0.5346\n",
      "Epoch 41: Train Loss: 0.5048, Train Acc: 0.8585, Val Loss: 2.2663, Val Acc: 0.5215\n",
      "Epoch 42: Train Loss: 0.4935, Train Acc: 0.8623, Val Loss: 2.1607, Val Acc: 0.5418\n",
      "Epoch 43: Train Loss: 0.4545, Train Acc: 0.8734, Val Loss: 2.3102, Val Acc: 0.5283\n",
      "Epoch 44: Train Loss: 0.4221, Train Acc: 0.8822, Val Loss: 2.2735, Val Acc: 0.5400\n",
      "Epoch 45: Train Loss: 0.3933, Train Acc: 0.8900, Val Loss: 2.1871, Val Acc: 0.5459\n",
      "Epoch 46: Train Loss: 0.3717, Train Acc: 0.8964, Val Loss: 2.1686, Val Acc: 0.5455\n",
      "Epoch 47: Train Loss: 0.3498, Train Acc: 0.9034, Val Loss: 2.2305, Val Acc: 0.5389\n",
      "Epoch 48: Train Loss: 0.3134, Train Acc: 0.9140, Val Loss: 2.2143, Val Acc: 0.5444\n",
      "Epoch 49: Train Loss: 0.2968, Train Acc: 0.9194, Val Loss: 2.3558, Val Acc: 0.5353\n",
      "Epoch 50: Train Loss: 0.2738, Train Acc: 0.9260, Val Loss: 2.2398, Val Acc: 0.5452\n",
      "Saved new best model.\n",
      "Epoch 51: Train Loss: 0.2592, Train Acc: 0.9296, Val Loss: 2.2160, Val Acc: 0.5527\n",
      "Epoch 52: Train Loss: 0.2256, Train Acc: 0.9397, Val Loss: 2.2306, Val Acc: 0.5506\n",
      "Saved new best model.\n",
      "Epoch 53: Train Loss: 0.2053, Train Acc: 0.9461, Val Loss: 2.1734, Val Acc: 0.5591\n",
      "Epoch 54: Train Loss: 0.1792, Train Acc: 0.9543, Val Loss: 2.2414, Val Acc: 0.5495\n",
      "Epoch 55: Train Loss: 0.1770, Train Acc: 0.9547, Val Loss: 2.2032, Val Acc: 0.5530\n",
      "Epoch 56: Train Loss: 0.1486, Train Acc: 0.9635, Val Loss: 2.1826, Val Acc: 0.5589\n",
      "Saved new best model.\n",
      "Epoch 57: Train Loss: 0.1330, Train Acc: 0.9681, Val Loss: 2.1250, Val Acc: 0.5659\n",
      "Saved new best model.\n",
      "Epoch 58: Train Loss: 0.1162, Train Acc: 0.9730, Val Loss: 2.1146, Val Acc: 0.5729\n",
      "Epoch 59: Train Loss: 0.1008, Train Acc: 0.9780, Val Loss: 2.1215, Val Acc: 0.5671\n",
      "Saved new best model.\n",
      "Epoch 60: Train Loss: 0.0842, Train Acc: 0.9824, Val Loss: 2.0385, Val Acc: 0.5806\n",
      "Saved new best model.\n",
      "Epoch 61: Train Loss: 0.0638, Train Acc: 0.9882, Val Loss: 2.0209, Val Acc: 0.5835\n",
      "Epoch 62: Train Loss: 0.0564, Train Acc: 0.9902, Val Loss: 2.0765, Val Acc: 0.5781\n",
      "Saved new best model.\n",
      "Epoch 63: Train Loss: 0.0442, Train Acc: 0.9935, Val Loss: 1.9879, Val Acc: 0.5918\n",
      "Saved new best model.\n",
      "Epoch 64: Train Loss: 0.0358, Train Acc: 0.9955, Val Loss: 1.9592, Val Acc: 0.5932\n",
      "Epoch 65: Train Loss: 0.0320, Train Acc: 0.9964, Val Loss: 1.9700, Val Acc: 0.5885\n",
      "Saved new best model.\n",
      "Epoch 66: Train Loss: 0.0300, Train Acc: 0.9966, Val Loss: 1.9105, Val Acc: 0.5990\n",
      "Saved new best model.\n",
      "Epoch 67: Train Loss: 0.0237, Train Acc: 0.9979, Val Loss: 1.8663, Val Acc: 0.6021\n",
      "Epoch 68: Train Loss: 0.0202, Train Acc: 0.9986, Val Loss: 1.8770, Val Acc: 0.6006\n",
      "Saved new best model.\n",
      "Epoch 69: Train Loss: 0.0180, Train Acc: 0.9989, Val Loss: 1.8530, Val Acc: 0.6067\n",
      "Saved new best model.\n",
      "Epoch 70: Train Loss: 0.0169, Train Acc: 0.9991, Val Loss: 1.8404, Val Acc: 0.6112\n",
      "Epoch 71: Train Loss: 0.0164, Train Acc: 0.9993, Val Loss: 1.8422, Val Acc: 0.6050\n",
      "Epoch 72: Train Loss: 0.0151, Train Acc: 0.9993, Val Loss: 1.8295, Val Acc: 0.6088\n",
      "Epoch 73: Train Loss: 0.0143, Train Acc: 0.9994, Val Loss: 1.8202, Val Acc: 0.6067\n",
      "Saved new best model.\n",
      "Epoch 74: Train Loss: 0.0139, Train Acc: 0.9996, Val Loss: 1.8010, Val Acc: 0.6121\n",
      "Epoch 75: Train Loss: 0.0136, Train Acc: 0.9996, Val Loss: 1.7984, Val Acc: 0.6103\n",
      "Saved new best model.\n",
      "Epoch 76: Train Loss: 0.0130, Train Acc: 0.9996, Val Loss: 1.7962, Val Acc: 0.6122\n",
      "Epoch 77: Train Loss: 0.0134, Train Acc: 0.9995, Val Loss: 1.7843, Val Acc: 0.6114\n",
      "Epoch 78: Train Loss: 0.0130, Train Acc: 0.9996, Val Loss: 1.7987, Val Acc: 0.6121\n",
      "Saved new best model.\n",
      "Epoch 79: Train Loss: 0.0124, Train Acc: 0.9997, Val Loss: 1.8015, Val Acc: 0.6144\n",
      "Epoch 80: Train Loss: 0.0125, Train Acc: 0.9997, Val Loss: 1.7952, Val Acc: 0.6116\n",
      "Epoch 81: Train Loss: 0.0126, Train Acc: 0.9997, Val Loss: 1.7889, Val Acc: 0.6120\n",
      "Epoch 82: Train Loss: 0.0122, Train Acc: 0.9996, Val Loss: 1.7753, Val Acc: 0.6112\n",
      "Epoch 83: Train Loss: 0.0120, Train Acc: 0.9997, Val Loss: 1.7966, Val Acc: 0.6135\n",
      "Epoch 84: Train Loss: 0.0118, Train Acc: 0.9998, Val Loss: 1.7765, Val Acc: 0.6144\n",
      "Epoch 85: Train Loss: 0.0118, Train Acc: 0.9997, Val Loss: 1.7850, Val Acc: 0.6098\n",
      "Saved new best model.\n",
      "Epoch 86: Train Loss: 0.0116, Train Acc: 0.9997, Val Loss: 1.7837, Val Acc: 0.6151\n",
      "Saved new best model.\n",
      "Epoch 87: Train Loss: 0.0115, Train Acc: 0.9997, Val Loss: 1.7713, Val Acc: 0.6171\n",
      "Epoch 88: Train Loss: 0.0114, Train Acc: 0.9997, Val Loss: 1.7760, Val Acc: 0.6149\n",
      "Epoch 89: Train Loss: 0.0115, Train Acc: 0.9997, Val Loss: 1.7761, Val Acc: 0.6130\n",
      "Epoch 90: Train Loss: 0.0113, Train Acc: 0.9998, Val Loss: 1.7681, Val Acc: 0.6142\n",
      "Epoch 91: Train Loss: 0.0113, Train Acc: 0.9999, Val Loss: 1.7819, Val Acc: 0.6128\n",
      "Epoch 92: Train Loss: 0.0113, Train Acc: 0.9998, Val Loss: 1.7650, Val Acc: 0.6145\n",
      "Epoch 93: Train Loss: 0.0112, Train Acc: 0.9997, Val Loss: 1.7782, Val Acc: 0.6150\n",
      "Epoch 94: Train Loss: 0.0113, Train Acc: 0.9997, Val Loss: 1.7812, Val Acc: 0.6141\n",
      "Epoch 95: Train Loss: 0.0111, Train Acc: 0.9998, Val Loss: 1.7758, Val Acc: 0.6104\n",
      "Epoch 96: Train Loss: 0.0111, Train Acc: 0.9998, Val Loss: 1.7685, Val Acc: 0.6149\n",
      "Epoch 97: Train Loss: 0.0111, Train Acc: 0.9998, Val Loss: 1.7612, Val Acc: 0.6145\n",
      "Epoch 98: Train Loss: 0.0111, Train Acc: 0.9998, Val Loss: 1.7766, Val Acc: 0.6118\n",
      "Epoch 99: Train Loss: 0.0111, Train Acc: 0.9998, Val Loss: 1.7762, Val Acc: 0.6111\n",
      "Epoch 100: Train Loss: 0.0109, Train Acc: 0.9997, Val Loss: 1.7734, Val Acc: 0.6148\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"lr\": []\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = ResNet34().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0004)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(xb)\n",
    "        loss = loss_fn(yhat, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = torch.argmax(yhat, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(yhat, dim=1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            val_total += xb.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"train_acc\"].append(accuracy)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_acc\"].append(val_accuracy)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), \"ResNet34.pth\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8865c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'training_history.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "fig_loss = go.Figure()\n",
    "fig_loss.add_trace(go.Scatter(y=data[\"train_loss\"], mode='lines', name='Train Loss'))\n",
    "fig_loss.add_trace(go.Scatter(y=data[\"val_loss\"], mode='lines', name='Val Loss'))\n",
    "fig_loss.update_layout(title='Training and Validation Loss over Epochs',\n",
    "                       xaxis_title='Epoch',\n",
    "                       yaxis_title='Loss')\n",
    "\n",
    "fig_acc = go.Figure()\n",
    "fig_acc.add_trace(go.Scatter(y=data[\"train_acc\"], mode='lines', name='Train Accuracy'))\n",
    "fig_acc.add_trace(go.Scatter(y=data[\"val_acc\"], mode='lines', name='Val Accuracy'))\n",
    "fig_acc.update_layout(title='Training and Validation Accuracy over Epochs',\n",
    "                      xaxis_title='Epoch',\n",
    "                      yaxis_title='Accuracy')\n",
    "\n",
    "fig_loss.show()\n",
    "fig_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c8dfd",
   "metadata": {},
   "source": [
    "![Loss](figures/Loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04d0bc",
   "metadata": {},
   "source": [
    "![Accuracy](figures/Accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ff9bf",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Achieved a Top-1 validation accuracy of ~61%, which is a solid result on Tiny ImageNet given the small image resolution and large number of classes.\n",
    "- Saved the best-performing model as ResNet34.pth, and maintained a detailed training history.\n",
    "\n",
    "Overall, this project demonstrated how to adapt classical deep convolutional architectures to smaller scale image datasets, while achieving meaningful results.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7815483,
     "sourceId": 12394036,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25416.640304,
   "end_time": "2025-07-09T11:14:41.396005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-09T04:11:04.755701",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
