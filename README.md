# ML-Models-on-Datasets
Collection of machine learning models applied to datasets using scikit-learn.

| Project                                    | Description                                                                                                                                                                                                                                                                                                                                      |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Abalone Age Prediction**                 | Predicted the age of abalone based on physical measurements (length, diameter, height, whole weight, etc.), replacing the tedious ring-counting lab process. Used XGBoost, Random Forest, and SGDRegressor, achieving a test RMSE as low as \~2.01. Applied feature engineering, outlier removal, scaling, and Optuna for hyperparameter tuning. |
| **Invariant Mass from Dielectron Events**  | Regressed the invariant mass of electron pairs in proton-proton collisions using CMS CERN open data. Built physics-informed features (momentum magnitude, angular differences, charge indicators) and tuned XGBoost to achieve \~0.997 R² on the test set. Included extensive preprocessing to filter out non-physical events.                   |
| **Softmax Classifier on Iris (NumPy)**     | Implemented a multi-class logistic regression (softmax) classifier from scratch using only NumPy, trained on the Iris dataset. Used SGD and cross-entropy loss, achieving \~97.4% test accuracy, illustrating fundamental multiclass classification without ML libraries.                                                                        |
| **LeNet-5 on MNIST (NumPy & PyTorch)**     | Built the classic LeNet-5 CNN entirely from scratch in NumPy, including manual forward/backward for conv, pooling, and FC layers. Couldn’t train fully due to CPU speed, so replicated in PyTorch to achieve \~98% on MNIST. Gave deep insight into how frameworks implement CNNs.                                                               |
| **Exoplanet Detection (Kepler)**           | Classified NASA Kepler objects as true exoplanets or false positives using LightGBM, XGBoost, Random Forest, and SGD. Conducted careful leakage removal and preprocessing. Tuned models to achieve \~95% test accuracy. Used 5-fold CV and plotted correlation heatmaps.                                                                         |
| **Mental Health Depression Detection**     | Predicted depression from a large-scale survey dataset using manual feature engineering and a ColumnTransformer pipeline. Applied XGBoost with Optuna tuning, achieving \~94% test accuracy. Included features from external data (crime rates) and handled diverse categorical variables.                                                       |
| **MLP on MNIST (NumPy)**                   | Built a fully connected neural network from scratch in NumPy with ReLU, softmax, cross-entropy, and explicit backprop. Used mini-batch gradient descent and He initialization, achieving \~98.3% accuracy on MNIST. A pure linear algebra implementation demonstrating how MLPs learn.                                                           |
| **Tiny ImageNet ResNet-34 (PyTorch)**      | Created a modified ResNet-34 from scratch in PyTorch to handle Tiny ImageNet’s small 64×64 images. Removed initial pooling and adjusted convolutions to prevent excessive downsampling. Achieved \~61% top-1 accuracy after 100 epochs, training on Kaggle GPUs.                                                                                 |
| **Stellar Classification (SDSS)**          | Classified stars, galaxies, and quasars using Sloan Digital Sky Survey data. Engineered extensive photometric features (like color differences, entropy, curvature) to capture spectral properties. Tuned LightGBM, XGBoost, and CatBoost to achieve \~98.5% test accuracy. Handled class imbalance with SMOTE.                                  |
| **GoogLeNet from Scratch (Tiny ImageNet)** | (Not yet documented in your project folder README) — Built Inception v1 from scratch in PyTorch, modified for 64×64 Tiny ImageNet data by carefully slowing down spatial reductions. Drew architecture by hand, built inception modules, and verified tensor shapes layer by layer.                                                              |
