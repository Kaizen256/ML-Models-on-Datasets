{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcab8e66",
   "metadata": {},
   "source": [
    "# A Minimal Softmax Classifier From Scratch Using the Iris Dataset\n",
    "In this notebook, I build a softmax classifier from scratch using numpy. The classifier will learn to classify the three types of iris flowers: setosa, versicolor, and virginica.\n",
    "I made this in order to understand the math behind machine learning models and it was a huge help.\n",
    "I will add as many comments explaining what I am doing to demonstrate my understanding, and \n",
    "because it helps give me a better understanding. This project will be different from most of the others because of the amount of explanations I included.\n",
    "For clarification this was made entirely with my understanding. No tutorials were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "48663b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0ea0eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data['data']    # Shape: (150, 4) — 4 features: sepal length, sepal width, petal length, petal width\n",
    "y = data['target']  # Shape: (150,) — Integer labels: 0 = setosa, 1 = versicolor, 2 = virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb841865",
   "metadata": {},
   "source": [
    "# One-Hot Encoding of Labels\n",
    "To use softmax and cross-entropy properly, we need to one-hot encode the integer class labels.\n",
    "Instead of using integer 1 (e.g., class = 1), we represent it as a vector: [0, 1, 0].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b9dc4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot = np.zeros(shape=(y.shape[0], 3))   # Create an array of zeros with shape (150, 3)\n",
    "y_onehot[np.arange(y.size), y] = 1           # Set the appropriate class index to 1 for each sample\n",
    "y_onehot[0]                                  # Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "dac227dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.25, random_state=62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd4e58",
   "metadata": {},
   "source": [
    "We split 25% of the data (38 samples) into the test set:\n",
    "- `X_train.shape = (112, 4)`\n",
    "- `y_train.shape = (112, 3)`\n",
    "- `X_test.shape = (38, 4)`\n",
    "- `y_test.shape = (38, 3)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d7a04",
   "metadata": {},
   "source": [
    "We now define our softmax and cross-entropy functions to convert logits to probabilities and measure how wrong the predictions are. Using SGD so batch size will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b3078e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Softmax function to convert logits (Z) into probabilities.\n",
    "    Works element-wise across classes.\n",
    "\n",
    "    Example:\n",
    "    If Z = [2.0, 1.0, 0.1], softmax(Z) = [0.659, 0.242, 0.099]\n",
    "    \"\"\"\n",
    "    Z = np.exp(Z)\n",
    "    sum = Z.sum()\n",
    "    return Z / sum # Shape is still (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e189168",
   "metadata": {},
   "source": [
    "For the Loss Function, Cross-Entropy is used. Cross-Entropy uses the natural logarithm on the probabilities from the softmax function and then multiplies them by y (the correct answer). Only the log of the correct answer ends up being used since all other y's = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "dfe16bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Loss for a single training example.\n",
    "    Assumes y is one-hot encoded and yhat contains probabilities.\n",
    "    \"\"\"\n",
    "    return - np.sum(y * np.log(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc1db7",
   "metadata": {},
   "source": [
    "# Parameter Initialization\n",
    "Initialize weights and biases.\n",
    "- w: shape (3, 4) → 3 classes × 4 input features\n",
    "- b: shape (3,) → 1 bias per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "1050eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.72188236, 0.66838096, 0.62716875, 0.08227346],\n",
       "        [0.9810613 , 0.86350937, 0.99160996, 0.23015252],\n",
       "        [0.85203934, 0.42605142, 0.25747436, 0.66313353]]),\n",
       " array([0.85785875, 0.59830137, 0.95121131]))"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(3, 4)\n",
    "b = np.random.rand(3)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030c3a3",
   "metadata": {},
   "source": [
    "We loop through the training data one example at a time (stochastic gradient descent), and use the softmax function for multi-class probability prediction and cross-entropy loss to measure prediction error. The model has: \n",
    "- X_train of shape (N, D) → N samples, D features\n",
    "- w of shape (C, D) → C classes, D features\n",
    "- b of shape (C,) → one bias per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "61594aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 112.5391 | Accuracy: 0.5625\n",
      "Epoch 1 Loss: 78.2860 | Accuracy: 0.7232\n",
      "Epoch 2 Loss: 67.9412 | Accuracy: 0.7321\n",
      "Epoch 3 Loss: 61.8576 | Accuracy: 0.7411\n",
      "Epoch 4 Loss: 57.5247 | Accuracy: 0.7768\n",
      "Epoch 5 Loss: 54.1238 | Accuracy: 0.8036\n",
      "Epoch 6 Loss: 51.3053 | Accuracy: 0.8393\n",
      "Epoch 7 Loss: 48.8916 | Accuracy: 0.8482\n",
      "Epoch 8 Loss: 46.7799 | Accuracy: 0.8571\n",
      "Epoch 9 Loss: 44.9052 | Accuracy: 0.8571\n",
      "Epoch 10 Loss: 43.2231 | Accuracy: 0.8661\n",
      "Epoch 11 Loss: 41.7015 | Accuracy: 0.8750\n",
      "Epoch 12 Loss: 40.3162 | Accuracy: 0.8750\n",
      "Epoch 13 Loss: 39.0483 | Accuracy: 0.8839\n",
      "Epoch 14 Loss: 37.8827 | Accuracy: 0.8839\n",
      "Epoch 15 Loss: 36.8068 | Accuracy: 0.8929\n",
      "Epoch 16 Loss: 35.8104 | Accuracy: 0.9107\n",
      "Epoch 17 Loss: 34.8848 | Accuracy: 0.9107\n",
      "Epoch 18 Loss: 34.0225 | Accuracy: 0.9196\n",
      "Epoch 19 Loss: 33.2172 | Accuracy: 0.9196\n",
      "Epoch 20 Loss: 32.4633 | Accuracy: 0.9196\n",
      "Epoch 21 Loss: 31.7561 | Accuracy: 0.9196\n",
      "Epoch 22 Loss: 31.0912 | Accuracy: 0.9196\n",
      "Epoch 23 Loss: 30.4651 | Accuracy: 0.9196\n",
      "Epoch 24 Loss: 29.8743 | Accuracy: 0.9196\n",
      "Epoch 25 Loss: 29.3160 | Accuracy: 0.9286\n",
      "Epoch 26 Loss: 28.7876 | Accuracy: 0.9286\n",
      "Epoch 27 Loss: 28.2867 | Accuracy: 0.9286\n",
      "Epoch 28 Loss: 27.8112 | Accuracy: 0.9286\n",
      "Epoch 29 Loss: 27.3592 | Accuracy: 0.9286\n",
      "Epoch 30 Loss: 26.9290 | Accuracy: 0.9286\n",
      "Epoch 31 Loss: 26.5192 | Accuracy: 0.9375\n",
      "Epoch 32 Loss: 26.1281 | Accuracy: 0.9464\n",
      "Epoch 33 Loss: 25.7547 | Accuracy: 0.9464\n",
      "Epoch 34 Loss: 25.3976 | Accuracy: 0.9464\n",
      "Epoch 35 Loss: 25.0559 | Accuracy: 0.9464\n",
      "Epoch 36 Loss: 24.7286 | Accuracy: 0.9464\n",
      "Epoch 37 Loss: 24.4148 | Accuracy: 0.9464\n",
      "Epoch 38 Loss: 24.1136 | Accuracy: 0.9464\n",
      "Epoch 39 Loss: 23.8243 | Accuracy: 0.9464\n",
      "Epoch 40 Loss: 23.5461 | Accuracy: 0.9464\n",
      "Epoch 41 Loss: 23.2786 | Accuracy: 0.9375\n",
      "Epoch 42 Loss: 23.0209 | Accuracy: 0.9375\n",
      "Epoch 43 Loss: 22.7727 | Accuracy: 0.9375\n",
      "Epoch 44 Loss: 22.5334 | Accuracy: 0.9375\n",
      "Epoch 45 Loss: 22.3024 | Accuracy: 0.9375\n",
      "Epoch 46 Loss: 22.0794 | Accuracy: 0.9375\n",
      "Epoch 47 Loss: 21.8640 | Accuracy: 0.9375\n",
      "Epoch 48 Loss: 21.6557 | Accuracy: 0.9375\n",
      "Epoch 49 Loss: 21.4542 | Accuracy: 0.9375\n",
      "Epoch 50 Loss: 21.2592 | Accuracy: 0.9375\n",
      "Epoch 51 Loss: 21.0703 | Accuracy: 0.9464\n",
      "Epoch 52 Loss: 20.8873 | Accuracy: 0.9464\n",
      "Epoch 53 Loss: 20.7098 | Accuracy: 0.9554\n",
      "Epoch 54 Loss: 20.5377 | Accuracy: 0.9554\n",
      "Epoch 55 Loss: 20.3706 | Accuracy: 0.9554\n",
      "Epoch 56 Loss: 20.2084 | Accuracy: 0.9554\n",
      "Epoch 57 Loss: 20.0508 | Accuracy: 0.9554\n",
      "Epoch 58 Loss: 19.8977 | Accuracy: 0.9554\n",
      "Epoch 59 Loss: 19.7488 | Accuracy: 0.9554\n",
      "Epoch 60 Loss: 19.6039 | Accuracy: 0.9554\n",
      "Epoch 61 Loss: 19.4630 | Accuracy: 0.9554\n",
      "Epoch 62 Loss: 19.3257 | Accuracy: 0.9554\n",
      "Epoch 63 Loss: 19.1920 | Accuracy: 0.9554\n",
      "Epoch 64 Loss: 19.0618 | Accuracy: 0.9554\n",
      "Epoch 65 Loss: 18.9349 | Accuracy: 0.9554\n",
      "Epoch 66 Loss: 18.8111 | Accuracy: 0.9554\n",
      "Epoch 67 Loss: 18.6903 | Accuracy: 0.9554\n",
      "Epoch 68 Loss: 18.5725 | Accuracy: 0.9554\n",
      "Epoch 69 Loss: 18.4575 | Accuracy: 0.9554\n",
      "Epoch 70 Loss: 18.3453 | Accuracy: 0.9554\n",
      "Epoch 71 Loss: 18.2356 | Accuracy: 0.9554\n",
      "Epoch 72 Loss: 18.1284 | Accuracy: 0.9554\n",
      "Epoch 73 Loss: 18.0237 | Accuracy: 0.9554\n",
      "Epoch 74 Loss: 17.9213 | Accuracy: 0.9554\n",
      "Epoch 75 Loss: 17.8212 | Accuracy: 0.9554\n",
      "Epoch 76 Loss: 17.7233 | Accuracy: 0.9554\n",
      "Epoch 77 Loss: 17.6275 | Accuracy: 0.9554\n",
      "Epoch 78 Loss: 17.5337 | Accuracy: 0.9554\n",
      "Epoch 79 Loss: 17.4419 | Accuracy: 0.9554\n",
      "Epoch 80 Loss: 17.3519 | Accuracy: 0.9554\n",
      "Epoch 81 Loss: 17.2639 | Accuracy: 0.9554\n",
      "Epoch 82 Loss: 17.1776 | Accuracy: 0.9554\n",
      "Epoch 83 Loss: 17.0930 | Accuracy: 0.9554\n",
      "Epoch 84 Loss: 17.0101 | Accuracy: 0.9554\n",
      "Epoch 85 Loss: 16.9289 | Accuracy: 0.9554\n",
      "Epoch 86 Loss: 16.8492 | Accuracy: 0.9554\n",
      "Epoch 87 Loss: 16.7710 | Accuracy: 0.9554\n",
      "Epoch 88 Loss: 16.6943 | Accuracy: 0.9554\n",
      "Epoch 89 Loss: 16.6190 | Accuracy: 0.9554\n",
      "Epoch 90 Loss: 16.5451 | Accuracy: 0.9554\n",
      "Epoch 91 Loss: 16.4726 | Accuracy: 0.9554\n",
      "Epoch 92 Loss: 16.4014 | Accuracy: 0.9554\n",
      "Epoch 93 Loss: 16.3314 | Accuracy: 0.9554\n",
      "Epoch 94 Loss: 16.2627 | Accuracy: 0.9554\n",
      "Epoch 95 Loss: 16.1952 | Accuracy: 0.9554\n",
      "Epoch 96 Loss: 16.1289 | Accuracy: 0.9554\n",
      "Epoch 97 Loss: 16.0637 | Accuracy: 0.9554\n",
      "Epoch 98 Loss: 15.9996 | Accuracy: 0.9554\n",
      "Epoch 99 Loss: 15.9366 | Accuracy: 0.9554\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0  # Sum of losses over the entire epoch\n",
    "    correct = 0     # Count of correctly predicted examples\n",
    "    for z in range(X_train.shape[0]):\n",
    "        x = X_train[z]          # Shape: (4,) — single training sample\n",
    "        raw = np.dot(w, x) + b  # Shape: (3,) — raw scores (logits) per class\n",
    "        yhat = softmax(raw)     # Shape: (3,) — predicted probabilities per class\n",
    "        \"\"\"\n",
    "        np.dot(w, x) performs matrix-vector multiplication,\n",
    "        w (3x4) · x (4,) into logits for 4 classes\n",
    "        \"\"\"\n",
    "\n",
    "        loss = CrossEntropy(yhat, y_train[z]) # Scalar loss for one sample\n",
    "        total_loss += loss                    # Accumulate loss\n",
    "\n",
    "        pred_class = np.argmax(yhat)        # Predicted class index\n",
    "        true_class = np.argmax(y_train[z])  # Ground truth class index\n",
    "        if pred_class == true_class:\n",
    "            correct += 1                    # Count if prediction is correct\n",
    "\n",
    "        dz = yhat - y_train[z] # Gradient of loss w.r.t. raw scores (softmax output - true labels)\n",
    "\n",
    "        dw = np.outer(dz, x) # Shape: (3, 4) — gradient of loss w.r.t. weights\n",
    "        db = dz              # Shape: (3,) — gradient of loss w.r.t. biases\n",
    "        w -= lr * dw         # SGD update for weights\n",
    "        b -= lr * db         # SGD update for biases\n",
    "\n",
    "        \"\"\"\n",
    "        dz gives the gradient of the cross-entropy loss.\n",
    "        np.outer(dz, x) computes the full gradient matrix for w.\n",
    "        And then the learning step subtracts the gradients scaled by the learning rate.\n",
    "        \"\"\"\n",
    "    acc = correct / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss:.4f} | Accuracy: {acc:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "06926ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(X_test.shape[0]):\n",
    "    pred = np.dot(w, X_test[i]) + b\n",
    "    yhat = softmax(pred)\n",
    "\n",
    "    pred_class = np.argmax(yhat)\n",
    "    true_class = np.argmax(y_test[i])\n",
    "    if pred_class == true_class:\n",
    "        correct += 1\n",
    "\n",
    "acc = correct / X_test.shape[0]\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1573a7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Our softmax classifier achieved 97.37% accuracy on the Iris test set.\n",
    "- The model was implemented entirely from scratch, without using any high-level ML libraries.\n",
    "- I used:\n",
    "  - The **softmax** function to convert logits to probabilities.\n",
    "  - **Cross-entropy loss** to penalize incorrect predictions.\n",
    "  - **Stochastic Gradient Descent** to update weights after each sample.\n",
    "\n",
    "This was a fun little project that solidified my understanding.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
