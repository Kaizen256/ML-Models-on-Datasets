{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fe4220",
   "metadata": {},
   "source": [
    "# MNIST Classification with a Multi-Layer Perceptron from Scratch (NumPy only)\n",
    "\n",
    "In this notebook, I implement a fully-connected neural network (MLP) from scratch using only NumPy.\n",
    "\n",
    "The goal is to classify handwritten digits from the MNIST dataset. The network architecture:\n",
    "Input (784) → Dense(128) → ReLU → Dense(64) → ReLU → Dense(10) → Softmax\n",
    "\n",
    "I created this project to strengthen my mathematical understanding of deep learning, no tutorials, no high-level ML libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db473f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "X = mnist['data']       # Shape: (70000, 784)\n",
    "y = mnist['target']     # Shape: (70000,)\n",
    "\n",
    "X = X / 255.0           # Normalize pixel values to [0, 1]\n",
    "y = y.astype(np.int32)  # Convert labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb53eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "X_test shape: (10000, 784)\n",
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test (60k train, 10k test)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0252a",
   "metadata": {},
   "source": [
    "# Define Activation and Loss Functions\n",
    "\n",
    "- ReLU introduces non-linearity.\n",
    "- Softmax turns logits into probabilities.\n",
    "- Cross-entropy penalizes incorrect class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "09a8b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Applies the ReLU function elementwise\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def ReLU_deriv(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc39ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Softmax function to convert logits (Z) into probabilities.\n",
    "    Works element-wise across classes.\n",
    "    \"\"\"\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Computes the mean Cross-Entropy loss for multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "    - yhat (np.ndarray): Predicted probabilities with shape (batch_size, num_classes)\n",
    "    - y (np.ndarray): One-hot encoded true labels with shape (batch_size, num_classes)\n",
    "    - eps (float): Small value to prevent log(0)\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean cross-entropy loss over the batch\n",
    "    \"\"\"\n",
    "    yhat = np.clip(yhat, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(y * np.log(yhat), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8105e",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "Input (784) → Dense(128) → ReLU → Dense(64) → ReLU → Output(10, softmax)\n",
    "\n",
    "- w1 shape = (128, 784)\n",
    "- b1 shape = (128,)\n",
    "- output shape = (bs, 128)\n",
    "\n",
    "- w2 shape = (64, 128)\n",
    "- b2 shape = (64,)\n",
    "- output shape = (bs, 64)\n",
    "\n",
    "- w3 shape = (10, 64)\n",
    "- b3 shape = (10,)\n",
    "- output shape = (bs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188289b5",
   "metadata": {},
   "source": [
    "# Initialize Weights and Biases with He Initialization\n",
    "He initialization helps with vanishing and exploding gradients. This was a problem before I included He initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb7d27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(128, 784) * np.sqrt(2 / 784)\n",
    "b1 = np.zeros(128)\n",
    "\n",
    "w2 = np.random.randn(64, 128) * np.sqrt(2 / 128)\n",
    "b2 = np.zeros(64)\n",
    "\n",
    "w3 = np.random.randn(10, 64) * np.sqrt(2 / 64)\n",
    "b3 = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ad0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.0077 | Accuracy: 0.9222\n",
      "Epoch 2 | Loss: 0.0035 | Accuracy: 0.9660\n",
      "Epoch 3 | Loss: 0.0026 | Accuracy: 0.9740\n",
      "Epoch 4 | Loss: 0.0020 | Accuracy: 0.9797\n",
      "Epoch 5 | Loss: 0.0016 | Accuracy: 0.9840\n",
      "Epoch 6 | Loss: 0.0015 | Accuracy: 0.9847\n",
      "Epoch 7 | Loss: 0.0012 | Accuracy: 0.9879\n",
      "Epoch 8 | Loss: 0.0010 | Accuracy: 0.9895\n",
      "Epoch 9 | Loss: 0.0011 | Accuracy: 0.9885\n",
      "Epoch 10 | Loss: 0.0009 | Accuracy: 0.9904\n",
      "Epoch 11 | Loss: 0.0009 | Accuracy: 0.9909\n",
      "Epoch 12 | Loss: 0.0008 | Accuracy: 0.9922\n",
      "Epoch 13 | Loss: 0.0007 | Accuracy: 0.9935\n",
      "Epoch 14 | Loss: 0.0006 | Accuracy: 0.9933\n",
      "Epoch 15 | Loss: 0.0007 | Accuracy: 0.9936\n",
      "Epoch 16 | Loss: 0.0006 | Accuracy: 0.9943\n",
      "Epoch 17 | Loss: 0.0006 | Accuracy: 0.9947\n",
      "Epoch 18 | Loss: 0.0006 | Accuracy: 0.9940\n",
      "Epoch 19 | Loss: 0.0006 | Accuracy: 0.9934\n",
      "Epoch 20 | Loss: 0.0006 | Accuracy: 0.9939\n",
      "Epoch 21 | Loss: 0.0008 | Accuracy: 0.9932\n",
      "Epoch 22 | Loss: 0.0005 | Accuracy: 0.9946\n",
      "Epoch 23 | Loss: 0.0005 | Accuracy: 0.9956\n",
      "Epoch 24 | Loss: 0.0006 | Accuracy: 0.9950\n",
      "Epoch 25 | Loss: 0.0005 | Accuracy: 0.9954\n",
      "Epoch 26 | Loss: 0.0007 | Accuracy: 0.9939\n",
      "Epoch 27 | Loss: 0.0006 | Accuracy: 0.9949\n",
      "Epoch 28 | Loss: 0.0004 | Accuracy: 0.9961\n",
      "Epoch 29 | Loss: 0.0004 | Accuracy: 0.9961\n",
      "Epoch 30 | Loss: 0.0002 | Accuracy: 0.9983\n",
      "Epoch 31 | Loss: 0.0003 | Accuracy: 0.9975\n",
      "Epoch 32 | Loss: 0.0004 | Accuracy: 0.9970\n",
      "Epoch 33 | Loss: 0.0005 | Accuracy: 0.9954\n",
      "Epoch 34 | Loss: 0.0003 | Accuracy: 0.9972\n",
      "Epoch 35 | Loss: 0.0002 | Accuracy: 0.9984\n",
      "Epoch 36 | Loss: 0.0001 | Accuracy: 0.9988\n",
      "Epoch 37 | Loss: 0.0000 | Accuracy: 0.9997\n",
      "Epoch 38 | Loss: 0.0000 | Accuracy: 0.9999\n",
      "Epoch 39 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 40 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 41 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 42 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 43 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 44 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 45 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 46 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 47 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 48 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 49 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 50 | Loss: 0.0000 | Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0    # Sum of losses for this epoch\n",
    "    correct = 0       # Count of correct predictions\n",
    "\n",
    "    # Shuffle data each epoch\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    \n",
    "    for batch in range(0, X_train.shape[0], 32):\n",
    "        X_train_batch = X_train[batch: batch + batch_size]  # (batch_size, 784)\n",
    "        y_train_batch = y_train[batch: batch + batch_size]  # (batch_size,)\n",
    "        batch_len = X_train_batch.shape[0]  # Actual batch size (handles last batch)\n",
    "\n",
    "        #Forward Pass\n",
    "        z1 = np.dot(X_train_batch, w1.T) + b1               # (batch_size, 128)\n",
    "        a1 = ReLU(z1)\n",
    "\n",
    "        z2 = np.dot(a1, w2.T) + b2                          # (batch_size, 64)\n",
    "        a2 = ReLU(z2)\n",
    "\n",
    "        z3 = np.dot(a2, w3.T) + b3                          # (batch_size, 10)\n",
    "        yhat = softmax(z3)                                  # (batch_size, 10)\n",
    "\n",
    "        # Onehot Encoding Labels\n",
    "        y_onehot = np.zeros((batch_len, 10))\n",
    "        y_onehot[np.arange(batch_len), y_train_batch] = 1   # Turn class indices into one-hot vectors\n",
    "\n",
    "        # Loss and Accuracy\n",
    "        loss = CrossEntropy(yhat, y_onehot)                 # Average loss over batch\n",
    "        total_loss += loss\n",
    "\n",
    "        preds = np.argmax(yhat, axis=1)                     # Class prediction per sample\n",
    "        correct += np.sum(preds == y_train_batch)           # Tally correct predictions\n",
    "\n",
    "        # Back Propagation\n",
    "        dz3 = yhat - y_onehot                               # Loss gradient\n",
    "        da2 = np.dot(dz3, w3)                               # Backprop into hidden layer 2\n",
    "        dz2 = da2 * ReLU_deriv(z2)\n",
    "\n",
    "        da1 = np.dot(dz2, w2)                               # Backprop into hidden layer 1\n",
    "        dz1 = da1 * ReLU_deriv(z1)\n",
    "\n",
    "        dw3 = np.dot(dz3.T, a2)\n",
    "        db3 = dz3.sum(axis=0)\n",
    "        dw2 = np.dot(dz2.T, a1)\n",
    "        db2 = dz2.sum(axis=0)\n",
    "        dw1 = np.dot(dz1.T, X_train_batch)\n",
    "        db1 = dz1.sum(axis=0)\n",
    "\n",
    "        # Adjusting Parameters\n",
    "        w3 -= lr * dw3\n",
    "        b3 -= lr * db3\n",
    "        w2 -= lr * dw2\n",
    "        b2 -= lr * db2\n",
    "        w1 -= lr * dw1\n",
    "        b1 -= lr * db1\n",
    "\n",
    "    # Epoch Results\n",
    "    acc = correct / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / X_train.shape[0]:.4f} | Accuracy: {acc:.4f}\") # Just realized I was displaying the loss wrong.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb4a94",
   "metadata": {},
   "source": [
    "Overfitting intentionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d9c1df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "# Forward pass on test set\n",
    "z1_test = np.dot(X_test, w1.T) + b1\n",
    "a1_test = ReLU(z1_test)\n",
    "\n",
    "z2_test = np.dot(a1_test, w2.T) + b2\n",
    "a2_test = ReLU(z2_test)\n",
    "\n",
    "z3_test = np.dot(a2_test, w3.T) + b3\n",
    "yhat_test = softmax(z3_test)\n",
    "\n",
    "# Predictions\n",
    "preds_test = np.argmax(yhat_test, axis=1)\n",
    "accuracy_test = np.mean(preds_test == y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadfa34",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Multi-layer perceptron (MLP) achieved 98.31% accuracy on the MNIST test set. The model was built entirely from scratch using NumPy, without any high-level libraries like PyTorch or TensorFlow. The ReLU activation to introduce non-linearity in hidden layers. The softmax function to convert the final layer’s outputs into class probabilities. Cross-entropy loss to measure prediction error for multi-class classification. Mini-batch gradient descent to update weights using backpropagation. He initialization to ensure stable training for deep ReLU networks.\n",
    "\n",
    "This was a more advanced follow-up to my softmax classifier project and helped deepen my understanding of how deep neural networks are trained, especially for backpropagation. This was a challenging but rewarding project, and I'm proud to have written every part myself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
