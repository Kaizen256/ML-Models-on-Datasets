┌────────────────────────────────────────┬────────────────────────────────────────────────────────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Import Path                            │ What it does / Purpose / When to use it                                │ Key Hyper-parameters                                                                                 │
├────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ sklearn.linear_model.SGDClassifier     │ • Large-scale or streaming linear classification (binary or multiclass)│ • loss – "hinge", "log_loss", "modified_huber", "perceptron" → controls objective                    │
│                                        │ • Supports SVM-like or logistic objectives with online updates         │ • penalty – "l2", "l1", "elasticnet" → regularization style                                          │
│                                        │ • Handles huge datasets and sparse inputs (e.g., TF-IDF)               │ • alpha – strength of regularization (e.g., 1e-6 to 1e-2)                                             │
│                                        │ • Use `partial_fit` for online or mini-batch learning                  │ • learning_rate – "optimal", "invscaling", "adaptive" + eta0, power_t                                │
│                                        │ • Use when LogisticRegression or LinearSVC dont scale                  │ • max_iter / tol – number of epochs & convergence threshold                                          │
│                                        │                                                                        │ • early_stopping + validation_fraction + n_iter_no_change → auto-stop on plateau                    │
│                                        │                                                                        │ • average – enables Polyak averaging for more stable updates                                         │
│                                        │                                                                        │ • class_weight – handle imbalanced classes                                                           │
│                                        │                                                                        │ • fit_intercept – set False if data is already centered                                              │
│                                        │                                                                        │ • shuffle – keep True to shuffle training each epoch                                                 │
│                                        │                                                                        │ • warm_start – allows continued training over multiple `.fit()` calls                                │
├────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ sklearn.linear_model.SGDRegressor      │ • Large-scale or online linear regression                              │ • loss – "squared_error", "huber", "epsilon_insensitive", "pinball", etc.                            │
│                                        │ • Used for real-time or streaming predictions (e.g., ad CTR, pricing)  │ • epsilon – width of margin for Huber / SVR-style losses                                             │
│                                        │ • Supports partial_fit and sparse inputs                               │ • eta0 / power_t – controls initial learning rate and decay                                          │
│                                        │ • Can model OLS, robust regression, and quantile (pinball) loss        │ • early_stopping + validation_fraction + n_iter_no_change                                            │
│                                        │                                                                        │ • average, penalty, alpha, l1_ratio, max_iter, tol, etc. → same as SGDClassifier                     │
├────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ xgboost.XGBClassifier                  │ • Gradient boosting for classification tasks                           │ • n_estimators – number of trees (typical range: 100–1000)                                          │
│                                        │ • Supports binary, multiclass, softprob outputs                        │ • learning_rate – shrinkage rate (0.01–0.3), smaller = slower but more stable                       │
│                                        │ • Highly accurate, often wins ML competitions                          │ • max_depth – tree depth (controls model complexity)                                                 │
│                                        │ • Handles missing values internally                                    │ • subsample – % of rows sampled per tree (0.5–1.0)                                                   │
│                                        │ • Use `use_label_encoder=False` in modern versions                     │ • colsample_bytree – % of columns sampled per tree                                                   │
│                                        │ • Works well with tabular / structured data                            │ • gamma – min loss reduction required to make a split                                                │
│                                        │ • GPU acceleration available via `tree_method='gpu_hist'`              │ • reg_alpha / reg_lambda – L1 and L2 regularization on weights                                       │
│                                        │                                                                        │ • scale_pos_weight – handle class imbalance (use: #neg / #pos)                                      │
│                                        │                                                                        │ • objective – e.g., "binary:logistic", "multi:softprob", "reg:squarederror"                         │
│                                        │                                                                        │ • eval_metric – logloss, error, auc, merror, mlogloss, etc.                                         │
│                                        │                                                                        │ • early_stopping_rounds – stop if no improvement in N eval rounds                                   │
│                                        │                                                                        │ • verbosity – control training log output (0 = silent, 1 = warning, 2 = info, 3 = debug)             │
├────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ xgboost.XGBRegressor                   │ • Gradient boosting for regression tasks                               │ • All hyperparameters from XGBClassifier apply here                                                 │
│                                        │ • Used for tabular numeric prediction (house prices, sales, etc.)      │ • objective – "reg:squarederror", "reg:squaredlogerror", "reg:pseudohubererror", "reg:gamma"        │
│                                        │ • Same speed and performance optimizations                             │ • eval_metric – rmse (default), mae, rmsle, mape, etc.                                              │
│                                        │ • Also supports `early_stopping_rounds` and custom eval sets           │                                                                                                     │
├────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤