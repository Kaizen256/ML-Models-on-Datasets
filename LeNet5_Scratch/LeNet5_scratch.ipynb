{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f1c7a1",
   "metadata": {},
   "source": [
    "LeNet5,, using original arhcitecture so even though ReLU and Maxpooling are better than Sigmoid and Averagepooling, we use the original functions. Instead of using the original LeNet uniform [-0.05, 0.05] initialization I am going to use Xavier initialization. \n",
    "\n",
    "Not going to implement stride in the convolutional function as it is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51abf37",
   "metadata": {},
   "source": [
    "| Layer      | Type                 | Parameters                             | Output Shape (input 28x28) |\n",
    "| ---------- | -------------------- | -------------------------------------- | -------------------------- |\n",
    "| **C1**     | Convolution          | 6 filters, 5×5 kernel, stride=1, pad=2 | (6, 28, 28)                |\n",
    "|            | Activation (Sigmoid) |                                        | (6, 28, 28)                |\n",
    "| **S2**     | Average Pooling      | 2×2 window, stride=2                   | (6, 14, 14)                |\n",
    "| **C3**     | Convolution          | 16 filters, 5×5 kernel, stride=1       | (16, 10, 10)               |\n",
    "|            | Activation (Sigmoid) |                                        | (16, 10, 10)               |\n",
    "| **S4**     | Average Pooling      | 2×2 window, stride=2                   | (16, 5, 5)                 |\n",
    "| **C5**     | Convolution          | 120 filters, 5×5 kernel, stride=1      | (120, 1, 1)                |\n",
    "|            | Activation (Sigmoid) |                                        | (120,)                     |\n",
    "| **F6**     | Fully Connected      | 120 → 84                               | (84,)                      |\n",
    "|            | Activation (Sigmoid) |                                        | (84,)                      |\n",
    "| **Output** | Fully Connected      | 84 → 10                                | (10,)                      |\n",
    "\n",
    "![Architecture](figures/Architecture.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc294866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "X = mnist['data']       # Shape: (70000, 784)\n",
    "y = mnist['target']     # Shape: (70000,)\n",
    "\n",
    "X = X / 255.0           # Normalize pixel values to [0, 1]\n",
    "y = y.astype(np.int32)  # Convert labels to integers\n",
    "\n",
    "X = X.reshape(-1, 1, 28, 28) # Reshape for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f3b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 1, 28, 28)\n",
      "X_test shape: (10000, 1, 28, 28)\n",
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test (60k train, 10k test)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abe2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f6db6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5533f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Softmax function to convert logits (Z) into probabilities.\n",
    "    Works element-wise across classes.\n",
    "    \"\"\"\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cab096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Computes the mean Cross-Entropy loss for multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "    - yhat (np.ndarray): Predicted probabilities with shape (batch_size, num_classes)\n",
    "    - y (np.ndarray): One-hot encoded true labels with shape (batch_size, num_classes)\n",
    "    - eps (float): Small value to prevent log(0)\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean cross-entropy loss over the batch\n",
    "    \"\"\"\n",
    "    yhat = np.clip(yhat, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(y * np.log(yhat), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a238f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X, p):\n",
    "    batch, channel, height, width = X.shape\n",
    "    Y = np.zeros(shape=(batch, channel, height+ p * 2, width + p * 2))\n",
    "    Y[:, :, p:p+height, p:p+width] = X\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851462d",
   "metadata": {},
   "source": [
    "This is going to get extremely ugly. I will try and explain what is going on but that for loop is tricky to simplify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efefc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convolution(X, p, in_channels, out_channels, K, bias):\n",
    "    \"\"\"\n",
    "    Performs a convolution over multi-channel input with multiple output channels.\n",
    "    \n",
    "    Arguments:\n",
    "    - X: input tensor of shape (batch, in_channels, height, width)\n",
    "    - padding: number of padding pixels on each side\n",
    "    - in_channels: number of input channels\n",
    "    - out_channels: number of output channels (number of filters)\n",
    "    - K: Kernel\n",
    "    - bias: Bias term added to each output channel\n",
    "    \n",
    "    Returns:\n",
    "    - Y: output tensor of shape (batch, out_channels, output_height, output_width)\n",
    "    - K: The kernel to be reused\n",
    "    - b: The bias to be reused\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad X\n",
    "    if p > 0:\n",
    "        X = padding(X, p)\n",
    "    batch, channel, height, width = X.shape\n",
    "\n",
    "    # Compute output size (assuming stride=1)\n",
    "    output_height = height - K.shape[2] + 1\n",
    "    output_width  = width - K.shape[3] + 1\n",
    "    Y = np.zeros(shape=(batch, out_channels, output_height, output_width))\n",
    "\n",
    "    \"\"\"\n",
    "    First loop attempt. Worked but was really slow.\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            for in_ch in range(in_channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        Y[b, out_ch, h, w] += np.sum(\n",
    "                            X[b, in_ch, h: h + K.shape[2], w: w + K.shape[3]] * \n",
    "                            K[out_ch, in_ch]\n",
    "            Y[b, out_ch, :, :] += bias[out_ch]\n",
    "    \"\"\"\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            accum = np.zeros((output_height, output_width))\n",
    "            # Sum over each input channel\n",
    "            for in_ch in range(in_channels):\n",
    "                # This uses a 2D sliding window, no inner h,w loop\n",
    "                for i in range(K.shape[2]):\n",
    "                    for j in range(K.shape[3]):\n",
    "                        accum += X[b, in_ch, i:i+output_height, j:j+output_width] * K[out_ch, in_ch, i, j]\n",
    "            # Add bias\n",
    "            Y[b, out_ch] = accum + bias[out_ch]\n",
    "\n",
    "    return Y, K, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25b7fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBackward(dY, X, K):\n",
    "    \"\"\"\n",
    "    Robust backprop for a convolution layer.\n",
    "    \"\"\"\n",
    "    batch, in_channels, height, width = X.shape\n",
    "    out_channels, _, kH, kW = K.shape\n",
    "    out_height, out_width = dY.shape[2], dY.shape[3]\n",
    "\n",
    "    dK = np.zeros_like(K)\n",
    "    db = np.sum(dY, axis=(0,2,3)) / batch\n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            for in_ch in range(in_channels):\n",
    "                for i in range(kH):\n",
    "                    for j in range(kW):\n",
    "                        h_end = i + out_height\n",
    "                        w_end = j + out_width\n",
    "                        # clip if we overshoot the input dims\n",
    "                        if h_end > height or w_end > width:\n",
    "                            h_end = min(h_end, height)\n",
    "                            w_end = min(w_end, width)\n",
    "                            slice_h = h_end - i\n",
    "                            slice_w = w_end - j\n",
    "                            # also slice dY to match\n",
    "                            dY_slice = dY[b, out_ch, :slice_h, :slice_w]\n",
    "                            X_slice = X[b, in_ch, i:h_end, j:w_end]\n",
    "                        else:\n",
    "                            X_slice = X[b, in_ch, i:h_end, j:w_end]\n",
    "                            dY_slice = dY[b, out_ch]\n",
    "\n",
    "                        # gradient w.r.t. kernel weights\n",
    "                        dK[out_ch, in_ch, i, j] += np.sum(X_slice * dY_slice) / batch\n",
    "\n",
    "                        # gradient w.r.t. input\n",
    "                        dX[b, in_ch, i:h_end, j:w_end] += K[out_ch, in_ch, i, j] * dY_slice\n",
    "    return dX, dK, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9d9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pooling(X, kernel_size, stride):\n",
    "\n",
    "    batch, channel, height, width = X.shape\n",
    "    output_height = (height - kernel_size[0]) // stride + 1\n",
    "    output_width  = (width - kernel_size[1]) // stride + 1\n",
    "    Y = np.zeros(shape=(batch, channel, output_height, output_width))\n",
    "\n",
    "    \"\"\"\n",
    "    First attempt, too slow.\n",
    "    for b in range(batch):\n",
    "        for ch in range(channel):\n",
    "            for h in range(output_height):\n",
    "                for w in range(output_width):\n",
    "                    # Maxpooling is better but I am sticking to the default architecture aside from the Xavier Initialization\n",
    "                    # Calculated this from a piece of paper, helped me understand everything a lot better.\n",
    "                    Y[b, ch, h, w] = np.mean(X[b, ch, \n",
    "                                               (h * stride): (h * stride) + kernel_size[0],\n",
    "                                               (w * stride): (w * stride) + kernel_size[1]])\n",
    "    \"\"\"\n",
    "    window_sum = np.zeros((batch, channel, output_height, output_width))\n",
    "    for i in range(kernel_size[0]):\n",
    "        for j in range(kernel_size[1]):\n",
    "            window_sum += X[:, :, \n",
    "                            i:i+output_height*stride:stride, \n",
    "                            j:j+output_width*stride:stride]\n",
    "    Y = window_sum / (kernel_size[0] * kernel_size[1])\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e3d21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoolingBackward(dY, X, kernel_size, stride):\n",
    "    batch, channel, height, width = X.shape\n",
    "    output_height, output_width = dY.shape[2], dY.shape[3]\n",
    "    kH, kW = kernel_size\n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for b in range(batch):\n",
    "        for ch in range(channel):\n",
    "            for h in range(output_height):\n",
    "                for w in range(output_width):\n",
    "                    dX[b, ch,\n",
    "                       (h * stride):(h * stride) + kH,\n",
    "                       (w * stride):(w * stride) + kW] += dY[b, ch, h, w] / (kH * kW)\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df78a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size, n_in, n_out):\n",
    "    limit = np.sqrt(6 / (n_in + n_out))\n",
    "    return np.random.uniform(-limit, limit, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c49a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_in = in_channels * kernel_size[0] * kernel_size[1]\n",
    "# n_out = out_channels * kernel_size[0] * kernel_size[1]\n",
    "\n",
    "c1_kernel = xavier_init(size=(6, 1, 5, 5),n_in=25, n_out=150)\n",
    "c1_bias = np.zeros(6)\n",
    "\n",
    "c3_kernel = xavier_init(size=(16, 6, 5, 5),n_in=150, n_out=400)\n",
    "c3_bias = np.zeros(16)\n",
    "\n",
    "c5_kernel = xavier_init(size=(120, 16, 5, 5),n_in=400, n_out=3000)\n",
    "c5_bias = np.zeros(120)\n",
    "\n",
    "f6_weights = xavier_init((84, 120), n_in=120, n_out=84)\n",
    "f6_bias = np.zeros(84)\n",
    "\n",
    "out_weights = xavier_init((10, 84), n_in=84, n_out=10)\n",
    "out_bias = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c00be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m a3 \u001b[38;5;241m=\u001b[39m sigmoid(c3)\n\u001b[0;32m     37\u001b[0m s4 \u001b[38;5;241m=\u001b[39m Pooling(a3, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m c5, c5_kernel, c5_bias \u001b[38;5;241m=\u001b[39m \u001b[43mConvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc5_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc5_bias\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m a5 \u001b[38;5;241m=\u001b[39m sigmoid(c5)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m120\u001b[39m)\n\u001b[0;32m     46\u001b[0m f6 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a5, f6_weights\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m f6_bias\n",
      "Cell \u001b[1;32mIn[12], line 53\u001b[0m, in \u001b[0;36mConvolution\u001b[1;34m(X, p, in_channels, out_channels, K, bias)\u001b[0m\n\u001b[0;32m     51\u001b[0m                     accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X[b, in_ch, i:i\u001b[38;5;241m+\u001b[39moutput_height, j:j\u001b[38;5;241m+\u001b[39moutput_width] \u001b[38;5;241m*\u001b[39m K[out_ch, in_ch, i, j]\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Add bias\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m         Y[b, out_ch] \u001b[38;5;241m=\u001b[39m accum \u001b[38;5;241m+\u001b[39m bias[out_ch]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Y, K, bias\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0    # Sum of losses for this epoch\n",
    "    correct = 0       # Count of correct predictions\n",
    "\n",
    "    # Shuffle data each epoch\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    \n",
    "    for batch in range(0, X_train.shape[0], 4):\n",
    "        print(batch)\n",
    "        X_train_batch = X_train[batch: batch + batch_size]  # (batch_size, 784)\n",
    "        y_train_batch = y_train[batch: batch + batch_size]  # (batch_size,)\n",
    "        batch_len = X_train_batch.shape[0]  # Actual batch size (handles last batch)\n",
    "\n",
    "        # Forward Pass\n",
    "        c1, c1_kernel, c1_bias = Convolution(X_train_batch,\n",
    "                                             p=2, \n",
    "                                             in_channels=1, \n",
    "                                             out_channels=6,\n",
    "                                             K=c1_kernel,\n",
    "                                             bias=c1_bias)\n",
    "        a1 = sigmoid(c1)\n",
    "        s2 = Pooling(a1, kernel_size=(2, 2), stride=2)\n",
    "        c3, c3_kernel, c3_bias = Convolution(s2,\n",
    "                                             p=0,\n",
    "                                             in_channels=6,\n",
    "                                             out_channels=16,\n",
    "                                             K=c3_kernel,\n",
    "                                             bias=c3_bias\n",
    "                                             )\n",
    "        a3 = sigmoid(c3)\n",
    "        s4 = Pooling(a3, kernel_size=(2, 2), stride=2)\n",
    "        c5, c5_kernel, c5_bias = Convolution(s4,\n",
    "                                             p=0,\n",
    "                                             in_channels=16,\n",
    "                                             out_channels=120,\n",
    "                                             K=c5_kernel,\n",
    "                                             bias=c5_bias\n",
    "                                             )\n",
    "        a5 = sigmoid(c5).reshape(4, 120)\n",
    "        f6 = np.dot(a5, f6_weights.T) + f6_bias\n",
    "        a6 = sigmoid(f6)\n",
    "        output = np.dot(a6, out_weights.T) + out_bias\n",
    "        yhat = softmax(output)\n",
    "        # Onehot Encoding Labels\n",
    "        y_onehot = np.zeros((batch_len, 10))\n",
    "        y_onehot[np.arange(batch_len), y_train_batch] = 1   # Turn class indices into one-hot vectors\n",
    "\n",
    "        # Loss and Accuracy\n",
    "        loss = CrossEntropy(yhat, y_onehot)                 # Average loss over batch\n",
    "        total_loss += loss\n",
    "\n",
    "        preds = np.argmax(yhat, axis=1)                     # Class prediction per sample\n",
    "        correct += np.sum(preds == y_train_batch)           # Tally correct predictions\n",
    "\n",
    "        dz_output = yhat - y_onehot\n",
    "        # Grad for output layer\n",
    "        dw_out = np.dot(dz_output.T, a6) / batch_len  # shape (10, 84)\n",
    "        db_out = dz_output.sum(axis=0) / batch_len    # shape (10,)\n",
    "\n",
    "        # Propagate to hidden layer\n",
    "        da6 = np.dot(dz_output, out_weights)          # (batch, 84)\n",
    "        dz6 = da6 * sigmoid_deriv(f6)                 # (batch, 84)\n",
    "\n",
    "        # Grad for F6 layer\n",
    "        dw_f6 = np.dot(dz6.T, a5) / batch_len         # shape (84, 120)\n",
    "        db_f6 = dz6.sum(axis=0) / batch_len           # shape (84,)\n",
    "\n",
    "        # Propagate to C5 output\n",
    "        da5 = np.dot(dz6, f6_weights)                 # (batch, 120)\n",
    "        da5 = da5.reshape(batch_len, 120, 1, 1)       # for compatibility with conv\n",
    "\n",
    "        # Propagate through C5 layer\n",
    "        ds4, dK5, db5 = ConvBackward(da5, s4, c5_kernel)\n",
    "\n",
    "        # Propagate through S4\n",
    "        da3 = PoolingBackward(ds4, a3, (2,2), 2)\n",
    "\n",
    "        #Propagate through C3\n",
    "        ds2, dK3, db3 = ConvBackward(da3 * sigmoid_deriv(c3), s2, c3_kernel)\n",
    "\n",
    "        #Propagate through S2\n",
    "        da1 = PoolingBackward(ds2, a1, (2,2), 2)\n",
    "\n",
    "        #Propagate through C1\n",
    "        _, dK1, db1 = ConvBackward(da1 * sigmoid_deriv(c1), X_train_batch, c1_kernel)\n",
    "\n",
    "        # Adjusting Parameters\n",
    "        out_weights -= lr * dw_out\n",
    "        out_bias -= lr * db_out\n",
    "        f6_weights -= lr * dw_f6\n",
    "        f6_bias -= lr * db_f6\n",
    "        c5_kernel -= lr * dK5\n",
    "        c5_bias -= lr * db5\n",
    "        c3_kernel -= lr * dK3\n",
    "        c3_bias -= lr * db3\n",
    "        c1_kernel -= lr * dK1\n",
    "        c1_bias -= lr * db1\n",
    "\n",
    "    # Epoch Results\n",
    "    acc = correct / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / X_train.shape[0]:.4f} | Accuracy: {acc:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edada27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
